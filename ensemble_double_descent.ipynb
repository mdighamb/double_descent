{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71UzQV-Pp8yD"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "import torch\n",
        "from math import ceil\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "import functorch\n",
        "from functorch import combine_state_for_ensemble\n",
        "from functorch import vmap\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.metrics import zero_one_loss\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.datasets import fetch_openml\n",
        "import os, shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# os.chdir(\"/content/drive/My Drive/6.s898 Project/Project\")\n",
        "# !mkdir scikit_learn_data\n",
        "# !mkdir mlp-model\n",
        "# os.mkdir('ensemble-model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/6.s898 Project/Project\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGb8HNOvVpXQ",
        "outputId": "a0c74521-34f8-4ac9-b6dd-891c5b32ae03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_obj(obj, name):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_obj(name):\n",
        "    with open(name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "metadata": {
        "id": "lqeq80c721rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90bucKclvlz3"
      },
      "outputs": [],
      "source": [
        "def get_next_param_count(param_counts, losses, past_dd=False, alpha=2):\n",
        "    \"\"\"Predicts the next paramter count given a list of\n",
        "    prior parameter counts and losses. This works by fitting\n",
        "    a third degree polynomial to the param_counts (independent\n",
        "    variable) and the losses (dependent variable) and using the\n",
        "    polynomial's derivative to detect when the interpolation\n",
        "    threshold curve has been reached. This will make\n",
        "    the spacing between the parameter counts differ depending\n",
        "    on how close the model is to exhibiting double descent.\n",
        "\n",
        "    ...\n",
        "    Parameters\n",
        "    ----------\n",
        "    param_counts : list or np.array of ints\n",
        "        The list of prior parameter counts that the model was\n",
        "        trained over\n",
        "    losses : list or np.array of floats\n",
        "        The list of final losses for each model with\n",
        "        corresponding paramter counts\n",
        "    past_dd : bool\n",
        "        Flag to indicate whether the interpolation threshold\n",
        "        has been reached. Set to False by default.\n",
        "    alpha : float\n",
        "        Tuning paramter that increases or decreases the value of\n",
        "        the next parameter count\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    param_count : int\n",
        "        The next parameter count\n",
        "    past_dd : bool\n",
        "        Flag that indicates whether the interpolation threshold\n",
        "        has been reached. This should be used as the input for\n",
        "        the next iteration of this algorithm\n",
        "    \"\"\"\n",
        "\n",
        "    current_iter = param_counts[-1]\n",
        "\n",
        "    if type(losses) != np.ndarray:\n",
        "        losses = np.array(losses)\n",
        "\n",
        "    if type(param_counts) != np.ndarray:\n",
        "        param_counts = np.array(param_counts)\n",
        "\n",
        "    # Create weight vector\n",
        "    # We weight datapoints that are further away from\n",
        "    # the current parameter count less (1/n) depending on\n",
        "    # how many indices (n) away it is from the current one\n",
        "    w = np.arange(1,len(param_counts) + 1, 1)\n",
        "    w = w/w.max()\n",
        "\n",
        "    poly = np.polyfit(param_counts[:], losses[:], 3, w=w)\n",
        "\n",
        "    dy = 3*poly[0]*(current_iter - 10**-4)**2 + 2*poly[1]*(current_iter - 10**-4) + poly[2]\n",
        "\n",
        "    # The current iteration of this adaptive parameter\n",
        "    # count algorithm does not use the second derivative\n",
        "    # dy2 = 6*poly[0]*(current_iter - 10**-3) + 2*poly[1]\n",
        "\n",
        "    sgn = 1 if dy < 0 else 0\n",
        "\n",
        "    if sgn == 0:\n",
        "        past_dd = True\n",
        "\n",
        "    next_count = sgn*max(alpha*dy, 3) + 1\n",
        "\n",
        "    if sgn and past_dd:\n",
        "        return ceil(next_count) + current_iter + 10, past_dd\n",
        "\n",
        "    return ceil(next_count) + current_iter, past_dd\n",
        "\n",
        "\n",
        "def labels_to_vec(labels, classes=10):\n",
        "    \"\"\"Turns integer labels into one-hot vectors\n",
        "\n",
        "    ...\n",
        "    Parameters\n",
        "    ----------\n",
        "    labels : np.array\n",
        "        The list of integer labels {0, 1, ... N}\n",
        "    classes : int\n",
        "        The number of classes in the dataset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    label_vectors : np.array\n",
        "        Stack of one-hot vectors\n",
        "    \"\"\"\n",
        "\n",
        "    out = []\n",
        "    for label in labels:\n",
        "        vec = np.array([0] * classes)\n",
        "        vec[int(label)] = 1\n",
        "        out.append(vec)\n",
        "\n",
        "    return np.stack(out)\n",
        "\n",
        "def torch_zero_one_loss(outputs, labels):\n",
        "    \"\"\"TEMP DOCSTRING\"\"\"\n",
        "\n",
        "    return torch.div((torch.argmax(outputs, dim=1) != labels).sum().double(), len(labels))\n",
        "\n",
        "def sk_zero_one_loss(x, y):\n",
        "    \"\"\"Utils wrapper for sk-learn zero_one_loss\n",
        "\n",
        "    ...\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : np.array\n",
        "        List of values/vectors to compare\n",
        "    y : np.array\n",
        "        List of values/vector to compare\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    mse : np.array\n",
        "        Zero-one Loss of elements of x and y\n",
        "    \"\"\"\n",
        "\n",
        "    return zero_one_loss(x,y)\n",
        "\n",
        "def sk_mean_squared_error(x, y):\n",
        "    \"\"\"Utils wrapper for sk-learn mean_squared_error\n",
        "\n",
        "    ...\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : np.array\n",
        "        List of values/vectors to compare\n",
        "    y : np.array\n",
        "        List of values/vector to compare\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    mse : np.array\n",
        "        Mean squared error of elements of x and y\n",
        "    \"\"\"\n",
        "\n",
        "    return mean_squared_error(x, y)\n",
        "\n",
        "\n",
        "\n",
        "class TensorBoardUtils:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def matplotlib_imshow(self, img, one_channel=False):\n",
        "        if one_channel:\n",
        "            img = img.mean(dim=0)\n",
        "        img = img / 2 + 0.5     # unnormalize\n",
        "        npimg = img.cpu().numpy()\n",
        "        if one_channel:\n",
        "            plt.imshow(npimg, cmap=\"plasma\")\n",
        "        else:\n",
        "            plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "\n",
        "    def images_to_probs(self, net, images):\n",
        "        '''\n",
        "        Generates predictions and corresponding probabilities from a trained\n",
        "        network and a list of images\n",
        "        '''\n",
        "        output = net(images)\n",
        "        # convert output probabilities to predicted class\n",
        "        _, preds_tensor = torch.max(output, 1)\n",
        "        preds_tensor = preds_tensor.cpu()\n",
        "        preds = np.squeeze(preds_tensor.numpy())\n",
        "        return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
        "\n",
        "\n",
        "    def plot_classes_preds(self, net, images, labels):\n",
        "        '''\n",
        "        Generates matplotlib Figure using a trained network, along with images\n",
        "        and labels from a batch, that shows the network's top prediction along\n",
        "        with its probability, alongside the actual label, coloring this\n",
        "        information based on whether the prediction was correct or not.\n",
        "        Uses the \"images_to_probs\" function.\n",
        "        '''\n",
        "        preds, probs = images_to_probs(net, images)\n",
        "        classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "        # plot the images in the batch, along with predicted and true labels\n",
        "        fig = plt.figure(figsize=(12, 48))\n",
        "        for idx in np.arange(4):\n",
        "            ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
        "            matplotlib_imshow(images[idx], one_channel=True)\n",
        "            ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
        "                classes[preds[idx]],\n",
        "                probs[idx] * 100.0,\n",
        "                classes[labels[idx]]),\n",
        "                        color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
        "        return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bO_zAjVgZgK"
      },
      "outputs": [],
      "source": [
        "class TorchData:\n",
        "    \"\"\"This class contains the attributes that all datasets have in common.\n",
        "    All datasets will inherit from this class\n",
        "\n",
        "    ...\n",
        "    Attributes\n",
        "    ----------\n",
        "    train_loader : PyTorch Dataloader\n",
        "        The dataloader for the training set\n",
        "    train_loader : PyTorch Dataloader\n",
        "        The dataloader for the testing set\n",
        "    data_x_dim : int\n",
        "        The size of the x-dimension for each image in the dataset\n",
        "    data_y_dim : int\n",
        "        The size of the y-dimension for each image in the dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.train_loader = None\n",
        "        self.test_loader = None\n",
        "        self.data_x_dim = None\n",
        "        self.data_y_dim = None\n",
        "\n",
        "class MNIST(TorchData):\n",
        "    \"\"\"The MNIST Dataset (Handwritten Digits)\n",
        "\n",
        "    ...\n",
        "    Attributes\n",
        "    ----------\n",
        "    train_loader : PyTorch Dataloader\n",
        "        The dataloader for the training set\n",
        "    train_loader : PyTorch Dataloader\n",
        "        The dataloader for the testing set\n",
        "    data_x_dim : int\n",
        "        The size of the x-dimension for each image in the dataset\n",
        "    data_y_dim : int\n",
        "        The size of the y-dimension for each image in the dataset\n",
        "    train_batch_size : int\n",
        "        The number of training examples per batch\n",
        "    test_batch_size : int\n",
        "        The number of testing examples per batch\n",
        "    dataloaders : dict\n",
        "        A dictionary that contains the 2 dataloaders. The keys are\n",
        "        \"train\" and \"test\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, training_samples=4000, train_batch=128, test_batch=128):\n",
        "        print('Initializing MNIST')\n",
        "        self.train_batch_size = train_batch\n",
        "        self.test_batch_size = test_batch\n",
        "        self.training_samples = training_samples\n",
        "\n",
        "        self.train_loader = torch.utils.data.DataLoader(\n",
        "                                torchvision.datasets.MNIST('./data',\n",
        "                                   train=True,\n",
        "                                   download=True,\n",
        "                                   transform=torchvision.transforms.Compose([\n",
        "                                       torchvision.transforms.ToTensor(),\n",
        "                                       torchvision.transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "                                batch_size=self.train_batch_size, shuffle=True)\n",
        "\n",
        "        train_dataset = torch.utils.data.Subset(self.train_loader.dataset,\n",
        "                                                range(0, training_samples))\n",
        "\n",
        "        self.train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                        batch_size=self.train_batch_size,\n",
        "                                                        shuffle=True)\n",
        "\n",
        "        self.test_loader = torch.utils.data.DataLoader(\n",
        "                        torchvision.datasets.MNIST('./data',\n",
        "                           train=False,\n",
        "                           download=True,\n",
        "                           transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "                        batch_size=self.test_batch_size, shuffle=False)\n",
        "\n",
        "        test_dataset = torch.utils.data.Subset(self.test_loader.dataset,\n",
        "                                               range(0, int(training_samples//.75) - training_samples))\n",
        "\n",
        "        self.test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                                       batch_size=self.test_batch_size,\n",
        "                                                       shuffle=True)\n",
        "\n",
        "        # Only use a subset of the MNIST dataset for MLP\n",
        "        self.dataloaders = {'train': self.train_loader,\n",
        "                            'test': self.test_loader}\n",
        "\n",
        "        self.data_x_dim = self.train_loader.dataset[0][0].shape[1]\n",
        "        self.data_y_dim = self.train_loader.dataset[0][0].shape[2]\n",
        "        self.classes = list(set([self.dataloaders['train'].dataset[i][1] for i in range(len(self.dataloaders['train']))]))\n",
        "        self.num_classes = len(self.classes)\n",
        "        self.samples = len(self.train_loader.dataset)\n",
        "        self.dataset_sizes = {'train': self.samples, 'test': len(self.test_loader.dataset)}\n",
        "\n",
        "class SKLearnData:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def get_mnist(filename=None, samples=100):\n",
        "        \"\"\"Returns a subset of the the MNIST Dataset as numpy arrays\n",
        "\n",
        "        ...\n",
        "        Parameters\n",
        "        ----------\n",
        "        samples : int\n",
        "            The number of datapoints that the user wants to be\n",
        "            returned. The size of the returned validation set\n",
        "            will be samples/2\n",
        "        filename : str\n",
        "            The filename that the dataset will be saved to.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        X : np.array\n",
        "            Training set of 784 (28*28) dimensional vectors\n",
        "            that correspond to 28x28 MNIST images\n",
        "        y : np.array\n",
        "            Labels for each of the vectors in X\n",
        "        X_val : np.array\n",
        "            Training set of 784 (28*28) dimensional vectors\n",
        "            that correspond to 28x28 MNIST images\n",
        "        y_val : np.array\n",
        "            Labels for each of the vectors in X_val\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        if os.path.exists('scikit_learn_data/X_saved.npy') and os.path.exists('scikit_learn_data/Y_saved.npy'):\n",
        "            X = np.load('scikit_learn_data/X_saved.npy')\n",
        "            y = np.load('scikit_learn_data/Y_saved.npy', allow_pickle=True)\n",
        "        else:\n",
        "            print('Fetching MNIST')\n",
        "            X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
        "            np.save('scikit_learn_data/X_saved.npy', X)\n",
        "            np.save('scikit_learn_data/Y_saved.npy', y)\n",
        "\n",
        "        if (samples + samples//2) > X.shape[0] - 1:\n",
        "            samples = X.shape[0] - 1\n",
        "\n",
        "        X_val = X[samples + 1:(samples + samples//2)]\n",
        "        y_val = y[samples + 1:(samples + samples//2)]\n",
        "        X = X[:samples + 1]\n",
        "        y = y[:samples + 1]\n",
        "\n",
        "        return X, y, X_val, y_val\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/6.s898 Project/Project')"
      ],
      "metadata": {
        "id": "Cn8BVah8fmoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wXmz-oM59Q3"
      },
      "outputs": [],
      "source": [
        "class Plotter:\n",
        "    \"\"\"This class has tools for plotting the double descent curve for different models\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def plot_adaboost(self, collected_data):\n",
        "        \"\"\"Plot double descent with the dictionary returned after training the scikit-learn\n",
        "        Random Forest classifier. The plots are saved to the current directory\n",
        "\n",
        "        ...\n",
        "        Parameters\n",
        "        ----------\n",
        "        collected_data : dict\n",
        "            The dictionary obtained by running double_descent on the RandomForest model\n",
        "        \"\"\"\n",
        "\n",
        "        custom_ticks_label = []\n",
        "        custom_ticks_x = []\n",
        "        for i in range(len(collected_data['trees'])):\n",
        "\n",
        "            if (collected_data['trees'][i] in (1, 25)) or (collected_data['trees'][i] == 50 and collected_data['forests'][i] in (1, 10, 20, 30)):\n",
        "                if i < len(collected_data['trees']) - 1 and collected_data['trees'][i] == collected_data['trees'][i + 1] and collected_data['forests'][i] == collected_data['forests'][i + 1]:\n",
        "                  continue\n",
        "                custom_ticks_label.append(\n",
        "                    str(collected_data['trees'][i]) + ' / ' + str(collected_data['forests'][i]))\n",
        "                custom_ticks_x.append(i)\n",
        "\n",
        "        plt.figure(figsize=(40, 40))\n",
        "        fig, ax1 = plt.subplots()\n",
        "\n",
        "        dataset = collected_data['dataset']\n",
        "        samples = collected_data['samples']\n",
        "\n",
        "        plt.title(f'AdaBoost on {dataset}; {samples} samples')\n",
        "\n",
        "        color = 'tab:blue'\n",
        "        ax1.set_xlabel('Model parameters N_tree/N_forest')\n",
        "        ax1.set_ylabel('Squared Loss')\n",
        "        ax1.plot(range(len(collected_data['mse_loss'])), collected_data['mse_loss'], color=color)\n",
        "        ax1.set_ylim(0, max(collected_data['mse_loss']))\n",
        "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "        color = 'tab:orange'\n",
        "        ax2.plot(range(len(collected_data['train_loss'])), collected_data['train_loss'], color=color)\n",
        "        ax2.axes.yaxis.set_visible(False)\n",
        "        ax2.set_ylim(0, max(collected_data['train_loss']))\n",
        "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "\n",
        "\n",
        "        train = mlines.Line2D([], [], color='tab:orange',\n",
        "                                  markersize=15, label='Train')\n",
        "\n",
        "        test = mlines.Line2D([], [], color='tab:blue',\n",
        "                                  markersize=15, label='Test')\n",
        "\n",
        "        plt.legend(handles=[train, test])\n",
        "\n",
        "        plt.xticks(custom_ticks_x, custom_ticks_label)\n",
        "\n",
        "        try:\n",
        "            os.mkdir('adaboost-figures')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            os.mkdir('adaboost-figures/squared_loss')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        directory = os.listdir('adaboost-figures/squared_loss')\n",
        "\n",
        "        path = f'adaboost-figures/squared_loss/dd_adaboost_squared_{len(directory)}.jpg'\n",
        "\n",
        "        plt.savefig(path)\n",
        "        plt.close()\n",
        "\n",
        "        # ---------------------------------------------------\n",
        "        plt.clf()\n",
        "        plt.figure(figsize=(40, 40))\n",
        "\n",
        "        fig, ax1 = plt.subplots()\n",
        "\n",
        "        dataset = collected_data['dataset']\n",
        "        samples = collected_data['samples']\n",
        "\n",
        "        plt.title(f'AdaBoost on {dataset}; {samples} samples')\n",
        "\n",
        "        color = 'tab:blue'\n",
        "        ax1.set_xlabel('Model parameters N_tree/N_forest')\n",
        "        ax1.set_ylabel('Zero-One Loss (%)')\n",
        "        ax1.plot(range(len(collected_data['zero_one_loss'])), collected_data['zero_one_loss'], color=color)\n",
        "        ax1.set_ylim(0, max(collected_data['zero_one_loss']))\n",
        "\n",
        "        ax2 = ax1.twinx()\n",
        "        color = 'tab:orange'\n",
        "        ax2.plot(range(len(collected_data['train_loss'])), collected_data['train_loss'], color=color)\n",
        "        ax2.axes.yaxis.set_visible(False)\n",
        "        ax2.set_ylim(0, max(collected_data['train_loss']))\n",
        "        fig.tight_layout()\n",
        "\n",
        "        train = mlines.Line2D([], [], color='tab:orange',\n",
        "                                  markersize=15, label='Train')\n",
        "\n",
        "        test = mlines.Line2D([], [], color='tab:blue',\n",
        "                                  markersize=15, label='Test')\n",
        "\n",
        "\n",
        "        plt.legend(handles=[train, test])\n",
        "\n",
        "        plt.xticks(custom_ticks_x, custom_ticks_label)\n",
        "\n",
        "\n",
        "        try:\n",
        "            os.mkdir('adaboost-figures')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            os.mkdir('adaboost-figures/zero_one')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        directory = os.listdir('adaboost-figures/zero_one')\n",
        "\n",
        "        path = f'adaboost-figures/zero_one/dd_adaboost_zero_one_{len(directory)}.jpg'\n",
        "\n",
        "        plt.savefig(path)\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "    def plot_l2boost(self, collected_data):\n",
        "        \"\"\"Plot double descent with the dictionary returned after training the scikit-learn\n",
        "        Random Forest classifier. The plots are saved to the current directory\n",
        "\n",
        "        ...\n",
        "        Parameters\n",
        "        ----------\n",
        "        collected_data : dict\n",
        "            The dictionary obtained by running double_descent on the RandomForest model\n",
        "        \"\"\"\n",
        "\n",
        "        custom_ticks_label = []\n",
        "        custom_ticks_x = []\n",
        "        for i in range(len(collected_data['trees'])):\n",
        "\n",
        "            if (collected_data['trees'][i] in (1, 25)) or (collected_data['trees'][i] == 50 and collected_data['forests'][i] in (1, 10, 20, 30)):\n",
        "                if i < len(collected_data['trees']) - 1 and collected_data['trees'][i] == collected_data['trees'][i + 1] and collected_data['forests'][i] == collected_data['forests'][i + 1]:\n",
        "                  continue\n",
        "                custom_ticks_label.append(\n",
        "                    str(collected_data['trees'][i]) + ' / ' + str(collected_data['forests'][i]))\n",
        "                custom_ticks_x.append(i)\n",
        "\n",
        "        plt.figure(figsize=(40, 40))\n",
        "        fig, ax1 = plt.subplots()\n",
        "\n",
        "        dataset = collected_data['dataset']\n",
        "        samples = collected_data['samples']\n",
        "\n",
        "        plt.title(f'L2-Boost on {dataset}; {samples} samples')\n",
        "\n",
        "        color = 'tab:blue'\n",
        "        ax1.set_xlabel('Model parameters N_tree/N_forest')\n",
        "        ax1.set_ylabel('Squared Loss')\n",
        "        ax1.plot(range(len(collected_data['mse_loss'])), collected_data['mse_loss'], color=color)\n",
        "        ax1.set_ylim(0, max(collected_data['mse_loss']))\n",
        "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "        color = 'tab:orange'\n",
        "        ax2.plot(range(len(collected_data['train_loss'])), collected_data['train_loss'], color=color)\n",
        "        ax2.axes.yaxis.set_visible(False)\n",
        "        ax2.set_ylim(0, max(collected_data['train_loss']))\n",
        "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "\n",
        "\n",
        "        train = mlines.Line2D([], [], color='tab:orange',\n",
        "                                  markersize=15, label='Train')\n",
        "\n",
        "        test = mlines.Line2D([], [], color='tab:blue',\n",
        "                                  markersize=15, label='Test')\n",
        "\n",
        "        plt.legend(handles=[train, test])\n",
        "\n",
        "        plt.xticks(custom_ticks_x, custom_ticks_label)\n",
        "\n",
        "        try:\n",
        "            os.mkdir('l2boost-figures')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            os.mkdir('l2boost-figures/squared_loss')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        directory = os.listdir('l2boost-figures/squared_loss')\n",
        "\n",
        "        path = f'l2boost-figures/squared_loss/dd_l2boost_squared_{len(directory)}.jpg'\n",
        "\n",
        "        plt.savefig(path)\n",
        "        plt.close()\n",
        "\n",
        "        # ---------------------------------------------------\n",
        "        plt.clf()\n",
        "        plt.figure(figsize=(40, 40))\n",
        "\n",
        "        fig, ax1 = plt.subplots()\n",
        "\n",
        "        dataset = collected_data['dataset']\n",
        "        samples = collected_data['samples']\n",
        "\n",
        "        plt.title(f'L2-Boost on {dataset}; {samples} samples')\n",
        "\n",
        "        color = 'tab:blue'\n",
        "        ax1.set_xlabel('Model parameters N_tree/N_forest')\n",
        "        ax1.set_ylabel('Zero-One Loss (%)')\n",
        "        ax1.plot(range(len(collected_data['zero_one_loss'])), collected_data['zero_one_loss'], color=color)\n",
        "        ax1.set_ylim(0, max(collected_data['zero_one_loss']))\n",
        "\n",
        "        ax2 = ax1.twinx()\n",
        "        color = 'tab:orange'\n",
        "        ax2.plot(range(len(collected_data['train_loss'])), collected_data['train_loss'], color=color)\n",
        "        ax2.axes.yaxis.set_visible(False)\n",
        "        ax2.set_ylim(0, max(collected_data['train_loss']))\n",
        "        fig.tight_layout()\n",
        "\n",
        "        train = mlines.Line2D([], [], color='tab:orange',\n",
        "                                  markersize=15, label='Train')\n",
        "\n",
        "        test = mlines.Line2D([], [], color='tab:blue',\n",
        "                                  markersize=15, label='Test')\n",
        "\n",
        "\n",
        "        plt.legend(handles=[train, test])\n",
        "\n",
        "        plt.xticks(custom_ticks_x, custom_ticks_label)\n",
        "\n",
        "\n",
        "        try:\n",
        "            os.mkdir('l2boost-figures')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            os.mkdir('l2boost-figures/zero_one')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        directory = os.listdir('l2boost-figures/zero_one')\n",
        "\n",
        "        path = f'l2boost-figures/zero_one/dd_l2boost_zero_one_{len(directory)}.jpg'\n",
        "\n",
        "        plt.savefig(path)\n",
        "        plt.close()\n",
        "\n",
        "    def plot_random_forest(self, collected_data):\n",
        "        \"\"\"Plot double descent with the dictionary returned after training the scikit-learn\n",
        "        Random Forest classifier. The plots are saved to the current directory\n",
        "\n",
        "        ...\n",
        "        Parameters\n",
        "        ----------\n",
        "        collected_data : dict\n",
        "            The dictionary obtained by running double_descent on the RandomForest model\n",
        "        \"\"\"\n",
        "\n",
        "        custom_ticks_label = []\n",
        "        custom_ticks_x = []\n",
        "        for i in range(len(collected_data['leaf_sizes'])):\n",
        "\n",
        "            if (collected_data['leaf_sizes'][i] in (10, 1010)) or (collected_data['leaf_sizes'][i] == 2010 and collected_data['trees'][i] in (1, 10, 20, 30)):\n",
        "                if i < len(collected_data['leaf_sizes']) - 1 and collected_data['leaf_sizes'][i] == collected_data['leaf_sizes'][i + 1] and collected_data['trees'][i] == collected_data['trees'][i + 1]:\n",
        "                  continue\n",
        "                custom_ticks_label.append(\n",
        "                    str(collected_data['leaf_sizes'][i]) + ' / ' + str(collected_data['trees'][i]))\n",
        "                custom_ticks_x.append(i)\n",
        "\n",
        "        plt.figure(figsize=(40, 40))\n",
        "        fig, ax1 = plt.subplots()\n",
        "\n",
        "        dataset = collected_data['dataset']\n",
        "        samples = collected_data['samples']\n",
        "\n",
        "        plt.title(f'Random Forest on {dataset}; {samples} samples')\n",
        "\n",
        "        color = 'tab:blue'\n",
        "        ax1.set_xlabel('Model parameters N_max_leaf/N_tree')\n",
        "        ax1.set_ylabel('Squared Loss')\n",
        "        ax1.plot(range(len(collected_data['mse_loss'])), collected_data['mse_loss'], color=color)\n",
        "        ax1.set_ylim(0, max(collected_data['mse_loss']))\n",
        "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "        color = 'tab:orange'\n",
        "        ax2.plot(range(len(collected_data['train_loss'])), collected_data['train_loss'], color=color)\n",
        "        ax2.axes.yaxis.set_visible(False)\n",
        "        ax2.set_ylim(0, max(collected_data['train_loss']))\n",
        "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "\n",
        "\n",
        "        train = mlines.Line2D([], [], color='tab:orange',\n",
        "                                  markersize=15, label='Train')\n",
        "\n",
        "        test = mlines.Line2D([], [], color='tab:blue',\n",
        "                                  markersize=15, label='Test')\n",
        "\n",
        "        plt.legend(handles=[train, test])\n",
        "\n",
        "        plt.xticks(custom_ticks_x, custom_ticks_label)\n",
        "\n",
        "        try:\n",
        "            os.mkdir('random-forest-figures')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            os.mkdir('random-forest-figures/squared_loss')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        directory = os.listdir('random-forest-figures/squared_loss')\n",
        "\n",
        "        path = f'random-forest-figures/squared_loss/dd_random_forest_squared_{len(directory)}.jpg'\n",
        "\n",
        "        plt.savefig(path)\n",
        "        plt.close()\n",
        "\n",
        "        # ---------------------------------------------------\n",
        "        plt.clf()\n",
        "        plt.figure(figsize=(40, 40))\n",
        "\n",
        "        fig, ax1 = plt.subplots()\n",
        "\n",
        "        dataset = collected_data['dataset']\n",
        "        samples = collected_data['samples']\n",
        "\n",
        "        plt.title(f'Random Forest on {dataset}; {samples} samples')\n",
        "\n",
        "        color = 'tab:blue'\n",
        "        ax1.set_xlabel('Model parameters N_max_leaf/N_tree')\n",
        "        ax1.set_ylabel('Zero-One Loss (%)')\n",
        "        ax1.plot(range(len(collected_data['zero_one_loss'])), collected_data['zero_one_loss'], color=color)\n",
        "        ax1.set_ylim(0, max(collected_data['zero_one_loss']))\n",
        "\n",
        "        ax2 = ax1.twinx()\n",
        "        color = 'tab:orange'\n",
        "        ax2.plot(range(len(collected_data['train_loss'])), collected_data['train_loss'], color=color)\n",
        "        ax2.axes.yaxis.set_visible(False)\n",
        "        ax2.set_ylim(0, max(collected_data['train_loss']))\n",
        "        fig.tight_layout()\n",
        "\n",
        "        train = mlines.Line2D([], [], color='tab:orange',\n",
        "                                  markersize=15, label='Train')\n",
        "\n",
        "        test = mlines.Line2D([], [], color='tab:blue',\n",
        "                                  markersize=15, label='Test')\n",
        "\n",
        "\n",
        "        plt.legend(handles=[train, test])\n",
        "\n",
        "        plt.xticks(custom_ticks_x, custom_ticks_label)\n",
        "\n",
        "\n",
        "        try:\n",
        "            os.mkdir('random-forest-figures')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            os.mkdir('random-forest-figures/zero_one')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        directory = os.listdir('random-forest-figures/zero_one')\n",
        "\n",
        "        path = f'random-forest-figures/zero_one/dd_rf_zero_one_{len(directory)}.jpg'\n",
        "\n",
        "        plt.savefig(path)\n",
        "        plt.close()\n",
        "\n",
        "    def plot_decisiontree(self, collected_data):\n",
        "        \"\"\"Plot double descent with the dictionary returned after training the scikit-learn\n",
        "        Random Forest classifier. The plots are saved to the current directory\n",
        "\n",
        "        ...\n",
        "        Parameters\n",
        "        ----------\n",
        "        collected_data : dict\n",
        "            The dictionary obtained by running double_descent on the RandomForest model\n",
        "        \"\"\"\n",
        "\n",
        "        custom_ticks_label = []\n",
        "        custom_ticks_x = []\n",
        "        for i in range(len(collected_data['leaf_sizes'])):\n",
        "\n",
        "            if (collected_data['leaf_sizes'][i] in (2, 102)) or (collected_data['leaf_sizes'][i] == 202 and collected_data['depths'][i] in (1, 51)) or (collected_data['leaf_sizes'][i] == 202 and collected_data['depths'][i] == 101 and collected_data['features'][i] in ('28', '420', '784')):\n",
        "                if i < len(collected_data['leaf_sizes']) - 1 and collected_data['leaf_sizes'][i] == collected_data['leaf_sizes'][i + 1] and collected_data['depths'][i] == collected_data['depths'][i + 1] and collected_data['features'][i] in ('sqrt', 28) and collected_data['features'][i + 1] in ('sqrt', 28):\n",
        "                  continue\n",
        "                if collected_data['features'][i] == 'sqrt':\n",
        "                  collected_data['features'][i] = 28\n",
        "                else:\n",
        "                  collected_data['features'][i] = int(collected_data['features'][i])\n",
        "                custom_ticks_label.append(\n",
        "                    str(collected_data['leaf_sizes'][i]) + '/' + str(collected_data['depths'][i]) + '/' + str(collected_data['features'][i]))\n",
        "                custom_ticks_x.append(i)\n",
        "\n",
        "        plt.figure(figsize=(40, 70))\n",
        "        fig, ax1 = plt.subplots()\n",
        "\n",
        "        dataset = collected_data['dataset']\n",
        "        samples = collected_data['samples']\n",
        "\n",
        "        plt.title(f'Decision Tree on {dataset}; {samples} samples')\n",
        "\n",
        "        color = 'tab:blue'\n",
        "        ax1.set_xlabel('Model parameters N_max_leaves/max_depth/max_features')\n",
        "        ax1.set_ylabel('Squared Loss')\n",
        "        ax1.plot(range(len(collected_data['mse_loss'])), collected_data['mse_loss'], color=color)\n",
        "        ax1.set_ylim(0, max(collected_data['mse_loss']))\n",
        "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "        color = 'tab:orange'\n",
        "        ax2.plot(range(len(collected_data['train_loss'])), collected_data['train_loss'], color=color)\n",
        "        ax2.axes.yaxis.set_visible(False)\n",
        "        ax2.set_ylim(0, max(collected_data['train_loss']))\n",
        "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "\n",
        "\n",
        "        train = mlines.Line2D([], [], color='tab:orange',\n",
        "                                  markersize=15, label='Train')\n",
        "\n",
        "        test = mlines.Line2D([], [], color='tab:blue',\n",
        "                                  markersize=15, label='Test')\n",
        "\n",
        "        plt.legend(handles=[train, test])\n",
        "\n",
        "        plt.xticks(custom_ticks_x, custom_ticks_label)\n",
        "\n",
        "        try:\n",
        "            os.mkdir('decision-tree-figures')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            os.mkdir('decision-tree-figures/squared_loss')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        directory = os.listdir('decision-tree-figures/squared_loss')\n",
        "\n",
        "        path = f'decision-tree-figures/squared_loss/dd_decision_tree_squared_{len(directory)}.jpg'\n",
        "\n",
        "        plt.savefig(path)\n",
        "        plt.close()\n",
        "\n",
        "        # ---------------------------------------------------\n",
        "        plt.clf()\n",
        "        plt.figure(figsize=(40, 40))\n",
        "\n",
        "        fig, ax1 = plt.subplots()\n",
        "\n",
        "        dataset = collected_data['dataset']\n",
        "        samples = collected_data['samples']\n",
        "\n",
        "        plt.title(f'Decision Tree on {dataset}; {samples} samples')\n",
        "\n",
        "        color = 'tab:blue'\n",
        "        ax1.set_xlabel('Model parameters N_max_leaves/max_depth/max_features')\n",
        "        ax1.set_ylabel('Zero-One Loss (%)')\n",
        "        ax1.plot(range(len(collected_data['zero_one_loss'])), collected_data['zero_one_loss'], color=color)\n",
        "        ax1.set_ylim(0, max(collected_data['zero_one_loss']))\n",
        "\n",
        "        ax2 = ax1.twinx()\n",
        "        color = 'tab:orange'\n",
        "        ax2.plot(range(len(collected_data['train_loss'])), collected_data['train_loss'], color=color)\n",
        "        ax2.axes.yaxis.set_visible(False)\n",
        "        ax2.set_ylim(0, max(collected_data['train_loss']))\n",
        "        fig.tight_layout()\n",
        "\n",
        "        train = mlines.Line2D([], [], color='tab:orange',\n",
        "                                  markersize=15, label='Train')\n",
        "\n",
        "        test = mlines.Line2D([], [], color='tab:blue',\n",
        "                                  markersize=15, label='Test')\n",
        "\n",
        "\n",
        "        plt.legend(handles=[train, test])\n",
        "\n",
        "        plt.xticks(custom_ticks_x, custom_ticks_label)\n",
        "\n",
        "\n",
        "        try:\n",
        "            os.mkdir('decision-tree-figures')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            os.mkdir('decision-tree-figures/zero_one')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        directory = os.listdir('decision-tree-figures/zero_one')\n",
        "\n",
        "        path = f'decision-tree-figures/zero_one/dd_decision_tree_zero_one_{len(directory)}.jpg'\n",
        "\n",
        "        plt.savefig(path)\n",
        "        plt.close()\n",
        "\n",
        "    def plot_logreg(self, collected_data):\n",
        "        \"\"\"Plot double descent with the dictionary returned after training the scikit-learn\n",
        "        Random Forest classifier. The plots are saved to the current directory\n",
        "\n",
        "        ...\n",
        "        Parameters\n",
        "        ----------\n",
        "        collected_data : dict\n",
        "            The dictionary obtained by running double_descent on the RandomForest model\n",
        "        \"\"\"\n",
        "\n",
        "        custom_ticks_label = []\n",
        "        custom_ticks_x = []\n",
        "        for i in tqdm(range(len(collected_data['sample_sizes']))):\n",
        "\n",
        "            if collected_data['sample_sizes'][i] in (7840, 3136, 1568, 784, 392, 196):\n",
        "                if collected_data['sample_sizes'][i] in (784, 392, 196):\n",
        "                  custom_ticks_label.append(str(784//collected_data['sample_sizes'][i]))\n",
        "                else:\n",
        "                  custom_ticks_label.append(str(784/collected_data['sample_sizes'][i]))\n",
        "                custom_ticks_x.append(i)\n",
        "\n",
        "        plt.figure(figsize=(40, 40))\n",
        "        fig, ax1 = plt.subplots()\n",
        "\n",
        "        dataset = collected_data['dataset']\n",
        "        # samples = collected_data['samples']\n",
        "\n",
        "        plt.title(f'Logistic Regression on {dataset}; varying samples')\n",
        "\n",
        "        color = 'tab:blue'\n",
        "        ax1.set_xlabel('Model parameters Features/Samples')\n",
        "        ax1.set_ylabel('Squared Loss')\n",
        "        ax1.plot(range(len(collected_data['mse_loss'])), collected_data['mse_loss'], color=color)\n",
        "        ax1.set_ylim(0, max(collected_data['mse_loss']))\n",
        "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "        color = 'tab:orange'\n",
        "        ax2.plot(range(len(collected_data['train_loss'])), collected_data['train_loss'], color=color)\n",
        "        ax2.axes.yaxis.set_visible(False)\n",
        "        ax2.set_ylim(0, max(collected_data['train_loss']))\n",
        "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "\n",
        "\n",
        "        train = mlines.Line2D([], [], color='tab:orange',\n",
        "                                  markersize=15, label='Train')\n",
        "\n",
        "        test = mlines.Line2D([], [], color='tab:blue',\n",
        "                                  markersize=15, label='Test')\n",
        "\n",
        "        plt.legend(handles=[train, test])\n",
        "\n",
        "        plt.xticks(custom_ticks_x, custom_ticks_label)\n",
        "\n",
        "        try:\n",
        "            os.mkdir('logistic-regression-dataset-figures')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            os.mkdir('logistic-regression-dataset-figures/squared_loss')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        directory = os.listdir('logistic-regression-dataset-figures/squared_loss')\n",
        "\n",
        "        path = f'logistic-regression-dataset-figures/squared_loss/dd_logistic_regression_squared_{len(directory)}.jpg'\n",
        "\n",
        "        plt.savefig(path)\n",
        "        plt.close()\n",
        "\n",
        "        # ---------------------------------------------------\n",
        "        plt.clf()\n",
        "        plt.figure(figsize=(40, 40))\n",
        "\n",
        "        fig, ax1 = plt.subplots()\n",
        "\n",
        "        dataset = collected_data['dataset']\n",
        "        # samples = collected_data['samples']\n",
        "\n",
        "        plt.title(f'Logistic Regression on {dataset}; varying samples')\n",
        "\n",
        "        color = 'tab:blue'\n",
        "        ax1.set_xlabel('Model parameters Features/Samples')\n",
        "        ax1.set_ylabel('Zero-One Loss (%)')\n",
        "        ax1.plot(range(len(collected_data['zero_one_loss'])), collected_data['zero_one_loss'], color=color)\n",
        "        ax1.set_ylim(0, max(collected_data['zero_one_loss']))\n",
        "\n",
        "        ax2 = ax1.twinx()\n",
        "        color = 'tab:orange'\n",
        "        ax2.plot(range(len(collected_data['train_loss'])), collected_data['train_loss'], color=color)\n",
        "        ax2.axes.yaxis.set_visible(False)\n",
        "        ax2.set_ylim(0, max(collected_data['train_loss']))\n",
        "        fig.tight_layout()\n",
        "\n",
        "        train = mlines.Line2D([], [], color='tab:orange',\n",
        "                                  markersize=15, label='Train')\n",
        "\n",
        "        test = mlines.Line2D([], [], color='tab:blue',\n",
        "                                  markersize=15, label='Test')\n",
        "\n",
        "\n",
        "        plt.legend(handles=[train, test])\n",
        "\n",
        "        plt.xticks(custom_ticks_x, custom_ticks_label)\n",
        "\n",
        "\n",
        "        try:\n",
        "            os.mkdir('logistic-regression-dataset-figures')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            os.mkdir('logistic-regression-dataset-figures/zero_one')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        directory = os.listdir('logistic-regression-dataset-figures/zero_one')\n",
        "\n",
        "        path = f'logistic-regression-dataset-figures/zero_one/dd_logistic_regression_zero_one_{len(directory)}.jpg'\n",
        "\n",
        "        plt.savefig(path)\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf = load_obj('logregD-dd-experiment')\n",
        "#rf['sample_sizes']\n",
        "plotter = Plotter()\n",
        "plotter.plot_logreg(rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "7PvqTh0L1fRW",
        "outputId": "fa69cf34-ffb5-47e6-9f90-34a9c51adfbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 40/40 [00:00<00:00, 47621.96it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 4000x4000 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 4000x4000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9ABcZluvm_P"
      },
      "outputs": [],
      "source": [
        "class TorchModels():\n",
        "    \"\"\"This class contains the attributes that all PyTorch models\n",
        "    have in common. All PyTorch models will inherit from this class\n",
        "\n",
        "    ...\n",
        "    Parameters (Not Attributes)\n",
        "    ---------------------------\n",
        "        cuda : bool\n",
        "            If True, the model will train using GPU acceleration if a CUDA\n",
        "            GPU is available. If False, the model will train on CPU\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        loss : str\n",
        "            The loss function for the model. Options are {'L1', 'MSE',\n",
        "            'CrossEntropy'}.\n",
        "        dataset : str\n",
        "            The dataset that the model will be trained on. Options are\n",
        "            {'MNIST'}.\n",
        "        cuda : bool\n",
        "            If True, the model will train using GPU acceleration if a CUDA\n",
        "            GPU is available. If False, the model will train on CPU\n",
        "        training_samples : int\n",
        "            Desired number of elements from the training set\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, loss, dataset, batch_size, training_samples, cuda):\n",
        "        super(TorchModels, self).__init__()\n",
        "\n",
        "        loss_functions = {'L1': nn.L1Loss(),\n",
        "                          'MSE': nn.MSELoss(),\n",
        "                          'CrossEntropy': nn.CrossEntropyLoss(),\n",
        "                          'NegativeLog':nn.NLLLoss()}\n",
        "\n",
        "        datasets = {'MNIST' : MNIST(training_samples=training_samples, train_batch=batch_size, test_batch=batch_size)}\n",
        "\n",
        "        self.loss = loss_functions[loss]\n",
        "        self.data = datasets[dataset]\n",
        "        self.cuda = cuda\n",
        "\n",
        "        if self.cuda and torch.cuda.is_available():\n",
        "            self.device = torch.device('cuda')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "\n",
        "\n",
        "class MultilayerPerceptron(TorchModels):\n",
        "    \"\"\"A wrapper for a multilayer perceptron with a single hidden layer of variable size\n",
        "\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "\n",
        "        loss : str\n",
        "            The loss function for the model. Options are {'L1', 'MSE',\n",
        "            'CrossEntropy'}.\n",
        "        dataset : str\n",
        "            The dataset that the model will be trained on. Options are\n",
        "            {'MNIST'}.\n",
        "        batch_size : int\n",
        "            The batch size for the training set\n",
        "        cuda : bool\n",
        "            If True, cuda will be used instead of cpu\n",
        "        optimizer : str\n",
        "            The optimizer that the model will use while training. Options are\n",
        "            {'SGD'}\n",
        "        learning_rate : float\n",
        "            Learning rate for optimizer\n",
        "        momentum : float\n",
        "            Momentum parameter to accelerate SGD\n",
        "        scheduler_step_size : int\n",
        "            Number of iterations before applying learning rate scheduler\n",
        "        gamma : float\n",
        "            Learning rate scheduler factor\n",
        "        current_count : int\n",
        "            The index of the current parameter count in param_counts\n",
        "        param_counts : np.array\n",
        "            List of parameter counts that the model will be trained over.\n",
        "            Since this model is an MLP, these counts correspond to N*10^3\n",
        "            neurons for a parameter count, N.\n",
        "        generate_parameters : True\n",
        "            Uses a parameter adaptation algorithm to predict the next best model\n",
        "            to train by analyzing the final loss vs hidden layer size of all previous\n",
        "            models\n",
        "        training_samples : int\n",
        "            Desired number of elements from the training set\n",
        "        factor : int\n",
        "            Multiplier for param_counts. factor * param_counts[i] = number\n",
        "            of neurons in hidden layer\n",
        "        reuse_weights : True\n",
        "            If True, reuses weights from previous model to help next model converge\n",
        "            more quickly\n",
        "        seed : int\n",
        "            Seed for random weight initialization\n",
        "        max_epochs : int\n",
        "            The max number of iterations to train each model\n",
        "    \"\"\"\n",
        "\n",
        "    class MLP(nn.Module):\n",
        "        \"\"\"An implementation of a 2-layer multilayer perceptron that allows\n",
        "        for changing the number of neurons in the hidden layer\"\"\"\n",
        "\n",
        "        def __init__(self, current_count, data, param_counts, factor, hidden_layer_size):\n",
        "            super().__init__()\n",
        "            print(f'Initializing MLP with {hidden_layer_size} hidden units')\n",
        "\n",
        "            self.data_dims = (data.data_x_dim, data.data_y_dim)\n",
        "\n",
        "            self.input_layer = nn.Linear(self.data_dims[0] * self.data_dims[1],\n",
        "                                         hidden_layer_size)\n",
        "\n",
        "            self.hidden_layer = nn.Linear(hidden_layer_size, data.num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x.view(-1, self.data_dims[0] * self.data_dims[1])\n",
        "            x = F.relu(self.input_layer(x))\n",
        "            x = self.hidden_layer(x)\n",
        "            return x\n",
        "\n",
        "\n",
        "    def __init__(self, loss='CrossEntropy',\n",
        "                 dataset='MNIST',\n",
        "                 batch_size=128,\n",
        "                 cuda=False,\n",
        "                 optimizer='SGD',\n",
        "                 learning_rate=.01,\n",
        "                 momentum=.95,\n",
        "                 scheduler_step_size=500,\n",
        "                 gamma=.9,\n",
        "                 current_count=0,\n",
        "                 param_counts=np.array([1, 2, 3]),\n",
        "                 generate_parameters=True,\n",
        "                 training_samples=4000,\n",
        "                 factor=10**3,\n",
        "                 reuse_weights=True,\n",
        "                 seed=None,\n",
        "                 max_epochs=1000):\n",
        "\n",
        "        super(MultilayerPerceptron, self).__init__(loss, dataset, batch_size, training_samples, cuda)\n",
        "\n",
        "        if seed:\n",
        "            torch.manual_seed(seed)\n",
        "        self.param_counts = param_counts\n",
        "        self.current_count = current_count\n",
        "        self.samples = training_samples\n",
        "        self.post_flag = 0\n",
        "        self.generate_parameters = generate_parameters\n",
        "        self.factor = factor\n",
        "        self.reuse_weights = reuse_weights\n",
        "        self.model = self.MLP(self.current_count, self.data, self.param_counts, self.factor, self.hidden_layer_size)\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.optim_dict = {'SGD': optim.SGD(self.model.parameters(),\n",
        "                                            lr=self.learning_rate,\n",
        "                                            momentum=self.momentum)}\n",
        "\n",
        "        self.optimizer = optimizer\n",
        "        self.mlp_optim = self.optim_dict[self.optimizer]\n",
        "        self.gamma = gamma\n",
        "        self.scheduler_step_size = scheduler_step_size\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.mlp_optim,\n",
        "                                                   step_size=self.scheduler_step_size,\n",
        "                                                   gamma=self.gamma)\n",
        "\n",
        "        self.losses = {'train': np.array([]),\n",
        "                       'test': np.array([]),\n",
        "                       'zero_one_train': np.array([]),\n",
        "                       'zero_one_test': np.array([])}\n",
        "        self.max_epochs = max_epochs\n",
        "\n",
        "\n",
        "    @property\n",
        "    def input_layer(self):\n",
        "        return self.model.input_layer\n",
        "\n",
        "\n",
        "    @property\n",
        "    def hidden_layer(self):\n",
        "        return self.model.hidden_layer\n",
        "\n",
        "    @property\n",
        "    def hidden_layer_size(self):\n",
        "        # This computes the size of the hidden layer, H, using the equation\n",
        "        # Total_Parameters = (d+1)*H + (H + 1)*K\n",
        "        return (self.param_counts[self.current_count] * self.factor \\\n",
        "                + self.data.num_classes)//(self.data.data_x_dim * self.data.data_y_dim + 1)\n",
        "\n",
        "    def reinitialize_classifier(self):\n",
        "        \"\"\"Uses new parameter count to initialize the next MLP.\n",
        "        The N weights from the previous model are transplanted into\n",
        "        the first N spots of the new model with M > N parameters\"\"\"\n",
        "\n",
        "        new_model = self.MLP(self.current_count, self.data, self.param_counts, self.factor, self.hidden_layer_size)\n",
        "\n",
        "        if self.reuse_weights:\n",
        "\n",
        "            in_weights = torch.randn_like(new_model.input_layer.weight)*.01\n",
        "            hidden_weights = torch.randn_like(new_model.hidden_layer.weight)*.01\n",
        "\n",
        "            in_weights[:self.model.input_layer.weight.shape[0]] = self.model.input_layer.weight\n",
        "            hidden_weights[:,:self.model.hidden_layer.weight.shape[1]] = self.model.hidden_layer.weight[:]\n",
        "\n",
        "            new_model.input_layer.weights = torch.nn.Parameter(data=in_weights)\n",
        "            new_model.hidden_layer.weights = torch.nn.Parameter(data=hidden_weights)\n",
        "\n",
        "        self.model = new_model\n",
        "\n",
        "        self.optim_dict = {'SGD': optim.SGD(self.model.parameters(),\n",
        "                                            lr=self.learning_rate,\n",
        "                                            momentum=self.momentum)}\n",
        "\n",
        "        self.mlp_optim = self.optim_dict[self.optimizer]\n",
        "\n",
        "        if self.param_counts[self.current_count] * self.factor > self.samples * self.data.num_classes:\n",
        "            self.gamma = 1\n",
        "\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.mlp_optim,\n",
        "                                                   step_size=self.scheduler_step_size,\n",
        "                                                   gamma=self.gamma)\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Trains the MLP model using the selected loss function,\n",
        "        optimizer, and scheduler. This also outputs to tensorboard.\n",
        "        To access all of the summaries for trained models, run the\n",
        "        tensorboard command in another command line while the model\n",
        "        is training\n",
        "\n",
        "        ...\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        model\n",
        "             A PyTorch neural network object that has been trained\n",
        "        train_loss : list\n",
        "            A list of all training losses at the end of each epoch\n",
        "        test_acc : list\n",
        "            A list of all test losses at the end of each epoch\n",
        "        zero_one_loss : list\n",
        "            A list of all 0-1 training losses at the end of each epoch\n",
        "        zero_one_acc : list\n",
        "            A list of all 0-1 test losses at the end of each epoch\n",
        "        \"\"\"\n",
        "\n",
        "        tb_utils = TensorBoardUtils()\n",
        "        model_writer = SummaryWriter(f'mlp-runs/dd_model_{self.param_counts[self.current_count]}')\n",
        "\n",
        "        # get some random training images\n",
        "        dataiter = iter(self.data.dataloaders['train'])\n",
        "        images, labels = next(dataiter)\n",
        "\n",
        "        # create grid of images\n",
        "        img_grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "        # show images\n",
        "        tb_utils.matplotlib_imshow(img_grid, one_channel=True)\n",
        "\n",
        "        # write to tensorboard\n",
        "        model_writer.add_image('MNIST Dataset', img_grid)\n",
        "        model_writer.add_graph(self.model, images)\n",
        "        model_writer.close()\n",
        "\n",
        "        train_loss = []\n",
        "        test_acc = []\n",
        "        zero_one_loss = []\n",
        "        zero_one_acc = []\n",
        "\n",
        "        print('Model with parameter count {}'.format(self.param_counts[self.current_count]))\n",
        "        print('-' * 10)\n",
        "\n",
        "        if self.cuda:\n",
        "            self.model = self.model.cuda()\n",
        "\n",
        "        for epoch in tqdm(range(self.max_epochs)):\n",
        "            # if epoch % 500 == 0:\n",
        "            #     print('Epoch {}/{}'.format(epoch + 1, self.max_epochs))\n",
        "            #     # print('-' * 10)\n",
        "\n",
        "            # Switches between training and testing sets\n",
        "            for phase in ['train', 'test']:\n",
        "\n",
        "                if phase == 'train':\n",
        "                    self.model.train()\n",
        "                    running_loss = 0.0\n",
        "                    running_zero_one_loss = 0.0\n",
        "                elif phase == 'test':\n",
        "                    self.model.eval()   # Set model to evaluate mode\n",
        "                    running_test_loss = 0.0\n",
        "                    running_zero_one_acc = 0.0\n",
        "\n",
        "                # Train/Test loop\n",
        "                for i, d in enumerate(self.data.dataloaders[phase], 0):\n",
        "\n",
        "                    inputs, labels = d\n",
        "\n",
        "                    if self.cuda:\n",
        "                        inputs = inputs.cuda()\n",
        "                        labels = labels.cuda()\n",
        "                    self.mlp_optim.zero_grad()\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        outputs = self.model.forward(inputs)\n",
        "                        loss = self.loss(outputs, labels)\n",
        "                        # backward + optimize only if in training phase\n",
        "                        loss.backward()\n",
        "                        self.mlp_optim.step()\n",
        "                        zero_one_train = torch_zero_one_loss(outputs, labels)\n",
        "                        running_zero_one_loss += zero_one_train.item() * inputs.size(0)\n",
        "                        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                    if phase == 'test':\n",
        "                        outputs = self.model.forward(inputs)\n",
        "                        test_loss = self.loss(outputs, labels)\n",
        "                        zero_one_test = torch_zero_one_loss(outputs, labels)\n",
        "                        running_zero_one_acc += zero_one_test.item() * inputs.size(0)\n",
        "                        running_test_loss += test_loss.item() * inputs.size(0)\n",
        "\n",
        "                if phase == 'train' and self.post_flag == False:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            train_loss.append(running_loss/ self.data.dataset_sizes['train'])\n",
        "            test_acc.append(running_test_loss/ self.data.dataset_sizes['test'])\n",
        "            zero_one_loss.append(running_zero_one_loss/self.data.dataset_sizes['train'])\n",
        "            zero_one_acc.append(running_zero_one_acc/self.data.dataset_sizes['test'])\n",
        "\n",
        "            model_writer.add_scalar(f'Train-Loss/{self.hidden_layer_size} hidden units; {self.param_counts[self.current_count]}*{self.factor} total parameters',\n",
        "                              train_loss[-1],\n",
        "                             epoch)\n",
        "\n",
        "            model_writer.add_scalar(f'Test-Loss/{self.hidden_layer_size} hidden units; {self.param_counts[self.current_count]}*{self.factor} total parameters',\n",
        "                              test_acc[-1],\n",
        "                              epoch)\n",
        "\n",
        "            model_writer.add_scalar(f'Train-Loss/{self.hidden_layer_size} hidden units; {self.param_counts[self.current_count]}*{self.factor} total parameters (Zero-One)',\n",
        "                                    zero_one_loss[-1],\n",
        "                                    epoch)\n",
        "\n",
        "            model_writer.add_scalar(f'Test-Loss/{self.hidden_layer_size} hidden units; {self.param_counts[self.current_count]}*{self.factor} total parameters (Zero-One)',\n",
        "                                    zero_one_acc[-1],\n",
        "                                    epoch)\n",
        "\n",
        "\n",
        "            if (zero_one_loss[-1] == 0 or train_loss[-1] < 10**-4):\n",
        "                if self.generate_parameters:\n",
        "                    if self.post_flag:\n",
        "                        break\n",
        "\n",
        "                if self.param_counts[self.current_count] * self.factor < self.samples * self.data.num_classes:\n",
        "                    break\n",
        "\n",
        "        print('Train Loss: {:.4f}\\nTest Loss {:.4f}\\n{} Hidden Units'.format(train_loss[-1], test_acc[-1], self.hidden_layer_size))\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return self.model, train_loss, test_acc, zero_one_loss, zero_one_acc\n",
        "\n",
        "\n",
        "    def double_descent(self):\n",
        "        \"\"\"Uses the train and get_next_param_count methods\n",
        "        to train the same architecture with varying parameter\n",
        "        sizes. This method also keeps track of the final losses\n",
        "        of each model that is trained\n",
        "\n",
        "        ...\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : Models instance\n",
        "            The model object that will be trained with varying\n",
        "            parameter sizes\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        try:\n",
        "            os.makedirs('mlp-output')\n",
        "        except Exception as E:\n",
        "            print('Could not make mlp-output')\n",
        "            print(E)\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            shutil.rmtree('mlp-runs')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        dd_writer = SummaryWriter('mlp-runs/double-descent')\n",
        "        while self.current_count < len(self.param_counts):\n",
        "\n",
        "            _, train_loss, test_loss, zero_one_train, zero_one_test = self.train()\n",
        "            save_obj(self, f'mlp-model/{self.param_counts[self.current_count]}_width')\n",
        "\n",
        "            self.losses['train'] = np.append(self.losses['train'], train_loss[-1])\n",
        "            self.losses['test'] = np.append(self.losses['test'], test_loss[-1])\n",
        "            self.losses['zero_one_train'] = np.append(self.losses['zero_one_train'], zero_one_train[-1])\n",
        "            self.losses['zero_one_test'] = np.append(self.losses['zero_one_test'], zero_one_test[-1])\n",
        "\n",
        "            dd_writer.add_scalar('MLP-Double-Descent/Train',\n",
        "                                 self.losses['train'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('MLP-Double-Descent/Test',\n",
        "                                 self.losses['test'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('MLP-Double-Descent/Train (Zero-One)',\n",
        "                                 self.losses['zero_one_train'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('MLP-Double-Descent/Test (Zero-One)',\n",
        "                                 self.losses['zero_one_test'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            self.current_count += 1\n",
        "\n",
        "            if self.current_count < len(self.param_counts):\n",
        "                self.reinitialize_classifier()\n",
        "\n",
        "            np.save('mlp-output/train_loss.npy', self.losses['train'])\n",
        "            np.save('mlp-output/test_loss.npy', self.losses['test'])\n",
        "            np.save('mlp-output/zero_one_train.npy', self.losses['zero_one_train'])\n",
        "            np.save('mlp-output/zero_one_test.npy', self.losses['zero_one_test'])\n",
        "            np.save('mlp-output/parameter_counts', self.param_counts)\n",
        "\n",
        "        if not self.generate_parameters:\n",
        "\n",
        "            return {'train_loss': self.losses['train'],\n",
        "                    'test_loss': self.losses['test'],\n",
        "                    'zero_one_train': self.losses['zero_one_train'],\n",
        "                    'zero_one_test': self.losses['zero_one_test'],\n",
        "                    'parameter_counts': self.param_counts}\n",
        "\n",
        "\n",
        "        self.current_count -= 1\n",
        "        flag = False\n",
        "        while self.post_flag < 4:\n",
        "\n",
        "\n",
        "            next_ct, flag = get_next_param_count(self.param_counts,\n",
        "                                                 self.losses['test']/self.losses['test'].sum(),\n",
        "                                                 flag)\n",
        "            save_obj(self, f'mlp-model/{next_ct}_width')\n",
        "            self.param_counts = np.append(self.param_counts, next_ct)\n",
        "            self.current_count += 1\n",
        "            self.reinitialize_classifier()\n",
        "\n",
        "            _, train_loss, test_loss, zero_one_train, zero_one_test = self.train()\n",
        "\n",
        "            self.losses['train'] = np.append(self.losses['train'], train_loss[-1])\n",
        "            self.losses['test'] = np.append(self.losses['test'], test_loss[-1])\n",
        "            self.losses['zero_one_train'] = np.append(self.losses['zero_one_train'], zero_one_train[-1])\n",
        "            self.losses['zero_one_test'] = np.append(self.losses['zero_one_test'], zero_one_test[-1])\n",
        "\n",
        "            dd_writer.add_scalar('MLP-Double-Descent/Train',\n",
        "                                 self.losses['train'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('MLP-Double-Descent/Test',\n",
        "                                 self.losses['test'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('MLP-Double-Descent/Train (Zero-One)',\n",
        "                                 self.losses['zero_one_train'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('MLP-Double-Descent/Test (Zero-One)',\n",
        "                                 self.losses['zero_one_test'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "\n",
        "            if flag and (self.param_counts[-1] - self.param_counts[-2]) != 1:\n",
        "                print('Iterating Post Flag')\n",
        "                self.post_flag += 1\n",
        "                print(f'Post Flag {self.post_flag}')\n",
        "\n",
        "            np.save('mlp-output/train_loss.npy', self.losses['train'])\n",
        "            np.save('mlp-output/test_loss.npy', self.losses['test'])\n",
        "            np.save('mlp-output/zero_one_train.npy', self.losses['zero_one_train'])\n",
        "            np.save('mlp-output/zero_one_test.npy', self.losses['zero_one_test'])\n",
        "            np.save('mlp-output/parameter_counts', self.param_counts)\n",
        "\n",
        "        return {'train_loss': self.losses['train'],\n",
        "                'test_loss': self.losses['test'],\n",
        "                'zero_one_train': self.losses['zero_one_train'],\n",
        "                'zero_one_test': self.losses['zero_one_test'],\n",
        "                'parameter_counts': self.param_counts}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnsembleMLP(TorchModels):\n",
        "    \"\"\"A wrapper for a multilayer perceptron with a single hidden layer of variable size\n",
        "\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "\n",
        "        loss : str\n",
        "            The loss function for the model. Options are {'L1', 'MSE',\n",
        "            'CrossEntropy'}.\n",
        "        dataset : str\n",
        "            The dataset that the model will be trained on. Options are\n",
        "            {'MNIST'}.\n",
        "        batch_size : int\n",
        "            The batch size for the training set\n",
        "        cuda : bool\n",
        "            If True, cuda will be used instead of cpu\n",
        "        optimizer : str\n",
        "            The optimizer that the model will use while training. Options are\n",
        "            {'SGD'}\n",
        "        learning_rate : float\n",
        "            Learning rate for optimizer\n",
        "        momentum : float\n",
        "            Momentum parameter to accelerate SGD\n",
        "        scheduler_step_size : int\n",
        "            Number of iterations before applying learning rate scheduler\n",
        "        gamma : float\n",
        "            Learning rate scheduler factor\n",
        "        current_count : int\n",
        "            The index of the current parameter count in param_counts\n",
        "        param_counts : np.array\n",
        "            List of parameter counts that the model will be trained over.\n",
        "            Since this model is an MLP, these counts correspond to N*10^3\n",
        "            neurons for a parameter count, N.\n",
        "        generate_parameters : True\n",
        "            Uses a parameter adaptation algorithm to predict the next best model\n",
        "            to train by analyzing the final loss vs hidden layer size of all previous\n",
        "            models\n",
        "        training_samples : int\n",
        "            Desired number of elements from the training set\n",
        "        factor : int\n",
        "            Multiplier for param_counts. factor * param_counts[i] = number\n",
        "            of neurons in hidden layer\n",
        "        reuse_weights : True\n",
        "            If True, reuses weights from previous model to help next model converge\n",
        "            more quickly\n",
        "        seed : int\n",
        "            Seed for random weight initialization\n",
        "        max_epochs : int\n",
        "            The max number of iterations to train each model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, loss='CrossEntropy',\n",
        "                 dataset='MNIST',\n",
        "                 batch_size=128,\n",
        "                 cuda=False,\n",
        "                 optimizer='SGD',\n",
        "                 learning_rate=.01,\n",
        "                 momentum=.95,\n",
        "                 scheduler_step_size=500,\n",
        "                 gamma=.9,\n",
        "                 current_count=0,\n",
        "                 param_counts=np.array([1, 2, 3]),\n",
        "                 generate_parameters=True,\n",
        "                 training_samples=4000,\n",
        "                 factor=10**3,\n",
        "                 reuse_weights=True,\n",
        "                 seed=None,\n",
        "                 max_epochs=1000,\n",
        "                 num_models=5):\n",
        "\n",
        "        super(EnsembleMLP, self).__init__(loss, dataset, batch_size, training_samples, cuda)\n",
        "\n",
        "        if seed:\n",
        "            torch.manual_seed(seed)\n",
        "        self.param_counts = param_counts\n",
        "        self.current_count = current_count\n",
        "        self.samples = training_samples\n",
        "        self.post_flag = 0\n",
        "        self.generate_parameters = generate_parameters\n",
        "        self.factor = factor\n",
        "        self.reuse_weights = reuse_weights\n",
        "        self.num_models = num_models\n",
        "        self.model = [MultilayerPerceptron.MLP(self.current_count, self.data, self.param_counts, self.factor, self.hidden_layer_size).to(\"cuda:0\") for _ in range(self.num_models)]\n",
        "        self.fmodel, self.params, self.buffers = combine_state_for_ensemble(self.model)\n",
        "        [p.requires_grad_() for p in self.params];\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.optim_dict = {'SGD': optim.SGD(self.params,\n",
        "                                            lr=self.learning_rate,\n",
        "                                            momentum=self.momentum)}\n",
        "\n",
        "        self.optimizer = optimizer\n",
        "        self.mlp_optim = self.optim_dict[self.optimizer]\n",
        "        self.gamma = gamma\n",
        "        self.scheduler_step_size = scheduler_step_size\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.mlp_optim,\n",
        "                                                   step_size=self.scheduler_step_size,\n",
        "                                                   gamma=self.gamma)\n",
        "\n",
        "        self.losses = {'train': np.array([]),\n",
        "                       'test': np.array([]),\n",
        "                       'zero_one_train': np.array([]),\n",
        "                       'zero_one_test': np.array([])}\n",
        "        self.max_epochs = max_epochs\n",
        "\n",
        "\n",
        "    @property\n",
        "    def input_layer(self):\n",
        "        return self.model.input_layer\n",
        "\n",
        "\n",
        "    @property\n",
        "    def hidden_layer(self):\n",
        "        return self.model.hidden_layer\n",
        "\n",
        "    @property\n",
        "    def hidden_layer_size(self):\n",
        "        # This computes the size of the hidden layer, H, using the equation\n",
        "        # Total_Parameters = (d+1)*H + (H + 1)*K\n",
        "        return (self.param_counts[self.current_count] * self.factor \\\n",
        "                + self.data.num_classes)//(self.data.data_x_dim * self.data.data_y_dim + 1)\n",
        "\n",
        "    def reinitialize_classifier(self):\n",
        "        \"\"\"Uses new parameter count to initialize the next MLP.\n",
        "        The N weights from the previous model are transplanted into\n",
        "        the first N spots of the new model with M > N parameters\"\"\"\n",
        "\n",
        "        new_model = [MultilayerPerceptron.MLP(self.current_count, self.data, self.param_counts, self.factor, self.hidden_layer_size).to('cuda') for _ in range(self.num_models)]\n",
        "\n",
        "        if self.reuse_weights:\n",
        "            for i in range(self.num_models):\n",
        "              in_weights = torch.randn_like(new_model[i].input_layer.weight)*.01\n",
        "              hidden_weights = torch.randn_like(new_model[i].hidden_layer.weight)*.01\n",
        "\n",
        "              in_weights[:self.model[i].input_layer.weight.shape[0]] = self.model[i].input_layer.weight\n",
        "              hidden_weights[:,:self.model[i].hidden_layer.weight.shape[1]] = self.model[i].hidden_layer.weight[:]\n",
        "\n",
        "              new_model[i].input_layer.weights = torch.nn.Parameter(data=in_weights)\n",
        "              new_model[i].hidden_layer.weights = torch.nn.Parameter(data=hidden_weights)\n",
        "\n",
        "        self.model = new_model\n",
        "        self.fmodel, self.params, self.buffers = combine_state_for_ensemble(self.model)\n",
        "        [p.requires_grad_() for p in self.params];\n",
        "        self.optim_dict = {'SGD': optim.SGD(self.params,\n",
        "                                            lr=self.learning_rate,\n",
        "                                            momentum=self.momentum)}\n",
        "\n",
        "        self.mlp_optim = self.optim_dict[self.optimizer]\n",
        "\n",
        "        if self.param_counts[self.current_count] * self.factor > self.samples * self.data.num_classes:\n",
        "            self.gamma = 1\n",
        "\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.mlp_optim,\n",
        "                                                   step_size=self.scheduler_step_size,\n",
        "                                                   gamma=self.gamma)\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Trains the MLP model using the selected loss function,\n",
        "        optimizer, and scheduler. This also outputs to tensorboard.\n",
        "        To access all of the summaries for trained models, run the\n",
        "        tensorboard command in another command line while the model\n",
        "        is training\n",
        "\n",
        "        ...\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        model\n",
        "             A PyTorch neural network object that has been trained\n",
        "        train_loss : list\n",
        "            A list of all training losses at the end of each epoch\n",
        "        test_acc : list\n",
        "            A list of all test losses at the end of each epoch\n",
        "        zero_one_loss : list\n",
        "            A list of all 0-1 training losses at the end of each epoch\n",
        "        zero_one_acc : list\n",
        "            A list of all 0-1 test losses at the end of each epoch\n",
        "        \"\"\"\n",
        "\n",
        "        tb_utils = TensorBoardUtils()\n",
        "        model_writer = SummaryWriter(f'ensemble-runs/dd_model_{self.param_counts[self.current_count]}_networks_{self.num_models}')\n",
        "\n",
        "        # get some random training images\n",
        "        dataiter = iter(self.data.dataloaders['train'])\n",
        "        images, labels = next(dataiter)\n",
        "\n",
        "        # create grid of images\n",
        "        img_grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "        # show images\n",
        "        tb_utils.matplotlib_imshow(img_grid, one_channel=True)\n",
        "\n",
        "        # write to tensorboard\n",
        "        # model_writer.add_image('MNIST Dataset', img_grid)\n",
        "        #model_writer.add_graph(self.model, images)\n",
        "        model_writer.close()\n",
        "\n",
        "        train_loss = []\n",
        "        test_acc = []\n",
        "        zero_one_loss = []\n",
        "        zero_one_acc = []\n",
        "\n",
        "        print('Model with parameter count {}'.format(self.param_counts[self.current_count]))\n",
        "        print('-' * 10)\n",
        "\n",
        "        if self.cuda:\n",
        "            for i in range(self.num_models):\n",
        "              self.model[i] = self.model[i].cuda()\n",
        "\n",
        "        for epoch in tqdm(range(self.max_epochs)):\n",
        "            # if epoch % 500 == 0:\n",
        "            #     print('Epoch {}/{}'.format(epoch + 1, self.max_epochs))\n",
        "            #     # print('-' * 10)\n",
        "\n",
        "            # Switches between training and testing sets\n",
        "            for phase in ['train', 'test']:\n",
        "\n",
        "                if phase == 'train':\n",
        "                    for i in range(self.num_models):\n",
        "                      self.model[i].train()\n",
        "                    running_loss = 0.0\n",
        "                    running_zero_one_loss = 0.0\n",
        "                elif phase == 'test':\n",
        "                    for i in range(self.num_models):\n",
        "                      self.model[i].eval()   # Set model to evaluate mode\n",
        "                    running_test_loss = 0.0\n",
        "                    running_zero_one_acc = 0.0\n",
        "\n",
        "                # Train/Test loop\n",
        "                for i, d in enumerate(self.data.dataloaders[phase], 0):\n",
        "\n",
        "                    inputs, labels = d\n",
        "\n",
        "                    if self.cuda:\n",
        "                        inputs = inputs.cuda()\n",
        "                        labels = labels.cuda()\n",
        "                    self.mlp_optim.zero_grad()\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        outputs = torch.mean(vmap(self.fmodel, (0, 0, None))(self.params, self.buffers, inputs), dim=0)\n",
        "                        loss = self.loss(outputs, labels)\n",
        "                        # backward + optimize only if in training phase\n",
        "                        loss.backward()\n",
        "                        self.mlp_optim.step()\n",
        "                        zero_one_train = torch_zero_one_loss(outputs, labels)\n",
        "                        running_zero_one_loss += zero_one_train.item() * inputs.size(0)\n",
        "                        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                    if phase == 'test':\n",
        "                        outputs = torch.mean(vmap(self.fmodel, (0, 0, None))(self.params, self.buffers, inputs), dim=0)\n",
        "                        test_loss = self.loss(outputs, labels)\n",
        "                        zero_one_test = torch_zero_one_loss(outputs, labels)\n",
        "                        running_zero_one_acc += zero_one_test.item() * inputs.size(0)\n",
        "                        running_test_loss += test_loss.item() * inputs.size(0)\n",
        "\n",
        "                if phase == 'train' and self.post_flag == False:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            train_loss.append(running_loss/ self.data.dataset_sizes['train'])\n",
        "            test_acc.append(running_test_loss/ self.data.dataset_sizes['test'])\n",
        "            zero_one_loss.append(running_zero_one_loss/self.data.dataset_sizes['train'])\n",
        "            zero_one_acc.append(running_zero_one_acc/self.data.dataset_sizes['test'])\n",
        "\n",
        "            model_writer.add_scalar(f'Train-Loss/{self.hidden_layer_size} hidden units; {self.param_counts[self.current_count]}*{self.factor} total parameters',\n",
        "                              train_loss[-1],\n",
        "                             epoch)\n",
        "\n",
        "            model_writer.add_scalar(f'Test-Loss/{self.hidden_layer_size} hidden units; {self.param_counts[self.current_count]}*{self.factor} total parameters',\n",
        "                              test_acc[-1],\n",
        "                              epoch)\n",
        "\n",
        "            model_writer.add_scalar(f'Train-Loss/{self.hidden_layer_size} hidden units; {self.param_counts[self.current_count]}*{self.factor} total parameters (Zero-One)',\n",
        "                                    zero_one_loss[-1],\n",
        "                                    epoch)\n",
        "\n",
        "            model_writer.add_scalar(f'Test-Loss/{self.hidden_layer_size} hidden units; {self.param_counts[self.current_count]}*{self.factor} total parameters (Zero-One)',\n",
        "                                    zero_one_acc[-1],\n",
        "                                    epoch)\n",
        "\n",
        "\n",
        "            if (zero_one_loss[-1] == 0 or train_loss[-1] < 10**-4):\n",
        "                if self.generate_parameters:\n",
        "                    if self.post_flag:\n",
        "                        break\n",
        "\n",
        "                if self.param_counts[self.current_count] * self.factor < self.samples * self.data.num_classes:\n",
        "                    break\n",
        "\n",
        "        print('Train Loss: {:.4f}\\nTest Loss {:.4f}\\n{} Hidden Units'.format(train_loss[-1], test_acc[-1], self.hidden_layer_size))\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return self.model, train_loss, test_acc, zero_one_loss, zero_one_acc\n",
        "\n",
        "\n",
        "    def double_descent(self):\n",
        "        \"\"\"Uses the train and get_next_param_count methods\n",
        "        to train the same architecture with varying parameter\n",
        "        sizes. This method also keeps track of the final losses\n",
        "        of each model that is trained\n",
        "\n",
        "        ...\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : Models instance\n",
        "            The model object that will be trained with varying\n",
        "            parameter sizes\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        try:\n",
        "            os.makedirs('ensemble-output')\n",
        "        except Exception as E:\n",
        "            print('Could not make ensemble-output')\n",
        "            print(E)\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            shutil.rmtree('ensemble-runs')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        dd_writer = SummaryWriter('ensemble-runs/double-descent')\n",
        "        while self.current_count < len(self.param_counts):\n",
        "\n",
        "            _, train_loss, test_loss, zero_one_train, zero_one_test = self.train()\n",
        "            save_obj(self, f'ensemble-model/{self.param_counts[self.current_count]}_width')\n",
        "\n",
        "            self.losses['train'] = np.append(self.losses['train'], train_loss[-1])\n",
        "            self.losses['test'] = np.append(self.losses['test'], test_loss[-1])\n",
        "            self.losses['zero_one_train'] = np.append(self.losses['zero_one_train'], zero_one_train[-1])\n",
        "            self.losses['zero_one_test'] = np.append(self.losses['zero_one_test'], zero_one_test[-1])\n",
        "\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Train',\n",
        "                                 self.losses['train'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Test',\n",
        "                                 self.losses['test'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Train (Zero-One)',\n",
        "                                 self.losses['zero_one_train'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Test (Zero-One)',\n",
        "                                 self.losses['zero_one_test'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            self.current_count += 1\n",
        "\n",
        "            if self.current_count < len(self.param_counts):\n",
        "                self.reinitialize_classifier()\n",
        "\n",
        "            np.save('ensemble-output/train_loss.npy', self.losses['train'])\n",
        "            np.save('ensemble-output/test_loss.npy', self.losses['test'])\n",
        "            np.save('ensemble-output/zero_one_train.npy', self.losses['zero_one_train'])\n",
        "            np.save('ensemble-output/zero_one_test.npy', self.losses['zero_one_test'])\n",
        "            np.save('ensemble-output/parameter_counts', self.param_counts)\n",
        "\n",
        "        if not self.generate_parameters:\n",
        "\n",
        "            return {'train_loss': self.losses['train'],\n",
        "                    'test_loss': self.losses['test'],\n",
        "                    'zero_one_train': self.losses['zero_one_train'],\n",
        "                    'zero_one_test': self.losses['zero_one_test'],\n",
        "                    'parameter_counts': self.param_counts}\n",
        "\n",
        "\n",
        "        self.current_count -= 1\n",
        "        flag = False\n",
        "        while self.post_flag < 4:\n",
        "\n",
        "\n",
        "            next_ct, flag = get_next_param_count(self.param_counts,\n",
        "                                                 self.losses['test']/self.losses['test'].sum(),\n",
        "                                                 flag)\n",
        "            save_obj(self, f'ensemble-model/{next_ct}_width')\n",
        "            self.param_counts = np.append(self.param_counts, next_ct)\n",
        "            self.current_count += 1\n",
        "            self.reinitialize_classifier()\n",
        "\n",
        "            _, train_loss, test_loss, zero_one_train, zero_one_test = self.train()\n",
        "\n",
        "            self.losses['train'] = np.append(self.losses['train'], train_loss[-1])\n",
        "            self.losses['test'] = np.append(self.losses['test'], test_loss[-1])\n",
        "            self.losses['zero_one_train'] = np.append(self.losses['zero_one_train'], zero_one_train[-1])\n",
        "            self.losses['zero_one_test'] = np.append(self.losses['zero_one_test'], zero_one_test[-1])\n",
        "\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Train',\n",
        "                                 self.losses['train'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Test',\n",
        "                                 self.losses['test'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Train (Zero-One)',\n",
        "                                 self.losses['zero_one_train'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Test (Zero-One)',\n",
        "                                 self.losses['zero_one_test'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "\n",
        "            if flag and (self.param_counts[-1] - self.param_counts[-2]) != 1:\n",
        "                print('Iterating Post Flag')\n",
        "                self.post_flag += 1\n",
        "                print(f'Post Flag {self.post_flag}')\n",
        "\n",
        "            np.save('ensemble-output/train_loss.npy', self.losses['train'])\n",
        "            np.save('ensemble-output/test_loss.npy', self.losses['test'])\n",
        "            np.save('ensemble-output/zero_one_train.npy', self.losses['zero_one_train'])\n",
        "            np.save('ensemble-output/zero_one_test.npy', self.losses['zero_one_test'])\n",
        "            np.save('ensemble-output/parameter_counts', self.param_counts)\n",
        "\n",
        "        return {'train_loss': self.losses['train'],\n",
        "                'test_loss': self.losses['test'],\n",
        "                'zero_one_train': self.losses['zero_one_train'],\n",
        "                'zero_one_test': self.losses['zero_one_test'],\n",
        "                'parameter_counts': self.param_counts}"
      ],
      "metadata": {
        "id": "NNeIQp3Kr6sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeakEnsemble(TorchModels):\n",
        "    \"\"\"A wrapper for a multilayer perceptron with a single hidden layer of variable size\n",
        "\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "\n",
        "        loss : str\n",
        "            The loss function for the model. Options are {'L1', 'MSE',\n",
        "            'CrossEntropy'}.\n",
        "        dataset : str\n",
        "            The dataset that the model will be trained on. Options are\n",
        "            {'MNIST'}.\n",
        "        batch_size : int\n",
        "            The batch size for the training set\n",
        "        cuda : bool\n",
        "            If True, cuda will be used instead of cpu\n",
        "        optimizer : str\n",
        "            The optimizer that the model will use while training. Options are\n",
        "            {'SGD'}\n",
        "        learning_rate : float\n",
        "            Learning rate for optimizer\n",
        "        momentum : float\n",
        "            Momentum parameter to accelerate SGD\n",
        "        scheduler_step_size : int\n",
        "            Number of iterations before applying learning rate scheduler\n",
        "        gamma : float\n",
        "            Learning rate scheduler factor\n",
        "        current_count : int\n",
        "            The index of the current parameter count in param_counts\n",
        "        param_counts : np.array\n",
        "            List of parameter counts that the model will be trained over.\n",
        "            Since this model is an MLP, these counts correspond to N*10^3\n",
        "            neurons for a parameter count, N.\n",
        "        generate_parameters : True\n",
        "            Uses a parameter adaptation algorithm to predict the next best model\n",
        "            to train by analyzing the final loss vs hidden layer size of all previous\n",
        "            models\n",
        "        training_samples : int\n",
        "            Desired number of elements from the training set\n",
        "        factor : int\n",
        "            Multiplier for param_counts. factor * param_counts[i] = number\n",
        "            of neurons in hidden layer\n",
        "        reuse_weights : True\n",
        "            If True, reuses weights from previous model to help next model converge\n",
        "            more quickly\n",
        "        seed : int\n",
        "            Seed for random weight initialization\n",
        "        max_epochs : int\n",
        "            The max number of iterations to train each model\n",
        "    \"\"\"\n",
        "\n",
        "    class MLP(nn.Module):\n",
        "        \"\"\"An implementation of a 2-layer multilayer perceptron that allows\n",
        "        for changing the number of neurons in the hidden layer\"\"\"\n",
        "\n",
        "        def __init__(self, current_count, data, param_counts, factor, hidden_layer_size):\n",
        "            super().__init__()\n",
        "            print(f'Initializing MLP with {hidden_layer_size} hidden units')\n",
        "\n",
        "            self.data_dims = (data.data_x_dim, data.data_y_dim)\n",
        "\n",
        "            self.input_layer = nn.Linear(self.data_dims[0] * self.data_dims[1],\n",
        "                                         hidden_layer_size)\n",
        "\n",
        "            self.hidden_layer = nn.Linear(hidden_layer_size, data.num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x.view(-1, self.data_dims[0] * self.data_dims[1])\n",
        "            x = F.relu(self.input_layer(x))\n",
        "            x = self.hidden_layer(x)\n",
        "            return x\n",
        "\n",
        "    def __init__(self, model1, model2, model3, model4, loss='CrossEntropy',\n",
        "                 dataset='MNIST',\n",
        "                 batch_size=128,\n",
        "                 cuda=False,\n",
        "                 optimizer='SGD',\n",
        "                 learning_rate=.01,\n",
        "                 momentum=.95,\n",
        "                 scheduler_step_size=500,\n",
        "                 gamma=.9,\n",
        "                 current_count=0,\n",
        "                 param_counts=np.array([1, 2, 3]),\n",
        "                 generate_parameters=True,\n",
        "                 training_samples=4000,\n",
        "                 factor=10**3,\n",
        "                 reuse_weights=True,\n",
        "                 seed=None,\n",
        "                 max_epochs=1000,\n",
        "                 num_votes=1):\n",
        "\n",
        "        super(WeakEnsemble, self).__init__(loss, dataset, batch_size, training_samples, cuda)\n",
        "\n",
        "        if seed:\n",
        "            torch.manual_seed(seed)\n",
        "        self.param_counts = param_counts\n",
        "        self.current_count = current_count\n",
        "        self.samples = training_samples\n",
        "        self.post_flag = 0\n",
        "        self.generate_parameters = generate_parameters\n",
        "        self.factor = factor\n",
        "        self.reuse_weights = reuse_weights\n",
        "        self.num_votes = num_votes\n",
        "        self.model = self.MLP(self.current_count, self.data, self.param_counts, self.factor, self.hidden_layer_size)\n",
        "        self.model1 = model1\n",
        "        self.model2 = model2\n",
        "        self.model3 = model3\n",
        "        self.model4 = model4\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.optim_dict = {'SGD': optim.SGD(self.model.parameters(),\n",
        "                                            lr=self.learning_rate,\n",
        "                                            momentum=self.momentum)}\n",
        "\n",
        "        self.optimizer = optimizer\n",
        "        self.mlp_optim = self.optim_dict[self.optimizer]\n",
        "        self.gamma = gamma\n",
        "        self.scheduler_step_size = scheduler_step_size\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.mlp_optim,\n",
        "                                                   step_size=self.scheduler_step_size,\n",
        "                                                   gamma=self.gamma)\n",
        "\n",
        "        self.losses = {'train': np.array([]),\n",
        "                       'test': np.array([]),\n",
        "                       'zero_one_train': np.array([]),\n",
        "                       'zero_one_test': np.array([])}\n",
        "        self.max_epochs = max_epochs\n",
        "\n",
        "    def predict_small(self, input):\n",
        "        np_input = input.view(input.shape[0], 784).cpu().numpy()\n",
        "        pred1 = labels_to_vec(self.model1.predict(np_input))\n",
        "        pred2 = labels_to_vec(self.model2.predict(np_input))\n",
        "        pred3 = labels_to_vec(self.model3.predict(np_input))\n",
        "        pred4 = labels_to_vec(stats.mode(np.array(\n",
        "                [self.model4[i].predict(np_input) for i in range(len(self.model4))]).astype(int), keepdims=True)[0].reshape(-1).astype(str))\n",
        "        return torch.from_numpy(pred1 + pred2 + pred3 + pred4).to(\"cuda\")\n",
        "\n",
        "\n",
        "    @property\n",
        "    def input_layer(self):\n",
        "        return self.model.input_layer\n",
        "\n",
        "\n",
        "    @property\n",
        "    def hidden_layer(self):\n",
        "        return self.model.hidden_layer\n",
        "\n",
        "    @property\n",
        "    def hidden_layer_size(self):\n",
        "        # This computes the size of the hidden layer, H, using the equation\n",
        "        # Total_Parameters = (d+1)*H + (H + 1)*K\n",
        "        return (self.param_counts[self.current_count] * self.factor \\\n",
        "                + self.data.num_classes)//(self.data.data_x_dim * self.data.data_y_dim + 1)\n",
        "\n",
        "    def reinitialize_classifier(self):\n",
        "        \"\"\"Uses new parameter count to initialize the next MLP.\n",
        "        The N weights from the previous model are transplanted into\n",
        "        the first N spots of the new model with M > N parameters\"\"\"\n",
        "\n",
        "        new_model = self.MLP(self.current_count, self.data, self.param_counts, self.factor, self.hidden_layer_size)\n",
        "\n",
        "        if self.reuse_weights:\n",
        "\n",
        "            in_weights = torch.randn_like(new_model.input_layer.weight)*.01\n",
        "            hidden_weights = torch.randn_like(new_model.hidden_layer.weight)*.01\n",
        "\n",
        "            in_weights[:self.model.input_layer.weight.shape[0]] = self.model.input_layer.weight\n",
        "            hidden_weights[:,:self.model.hidden_layer.weight.shape[1]] = self.model.hidden_layer.weight[:]\n",
        "\n",
        "            new_model.input_layer.weights = torch.nn.Parameter(data=in_weights)\n",
        "            new_model.hidden_layer.weights = torch.nn.Parameter(data=hidden_weights)\n",
        "\n",
        "        self.model = new_model\n",
        "\n",
        "        self.optim_dict = {'SGD': optim.SGD(self.model.parameters(),\n",
        "                                            lr=self.learning_rate,\n",
        "                                            momentum=self.momentum)}\n",
        "\n",
        "        self.mlp_optim = self.optim_dict[self.optimizer]\n",
        "\n",
        "        if self.param_counts[self.current_count] * self.factor > self.samples * self.data.num_classes:\n",
        "            self.gamma = 1\n",
        "\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.mlp_optim,\n",
        "                                                   step_size=self.scheduler_step_size,\n",
        "                                                   gamma=self.gamma)\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Trains the MLP model using the selected loss function,\n",
        "        optimizer, and scheduler. This also outputs to tensorboard.\n",
        "        To access all of the summaries for trained models, run the\n",
        "        tensorboard command in another command line while the model\n",
        "        is training\n",
        "\n",
        "        ...\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        model\n",
        "             A PyTorch neural network object that has been trained\n",
        "        train_loss : list\n",
        "            A list of all training losses at the end of each epoch\n",
        "        test_acc : list\n",
        "            A list of all test losses at the end of each epoch\n",
        "        zero_one_loss : list\n",
        "            A list of all 0-1 training losses at the end of each epoch\n",
        "        zero_one_acc : list\n",
        "            A list of all 0-1 test losses at the end of each epoch\n",
        "        \"\"\"\n",
        "\n",
        "        tb_utils = TensorBoardUtils()\n",
        "        model_writer = SummaryWriter(f'{self.num_votes}-ensemble-runs/dd_model_{self.param_counts[self.current_count]}')\n",
        "\n",
        "        # get some random training images\n",
        "        dataiter = iter(self.data.dataloaders['train'])\n",
        "        images, labels = next(dataiter)\n",
        "\n",
        "        # create grid of images\n",
        "        img_grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "        # show images\n",
        "        tb_utils.matplotlib_imshow(img_grid, one_channel=True)\n",
        "\n",
        "        # write to tensorboard\n",
        "        model_writer.add_image('MNIST Dataset', img_grid)\n",
        "        model_writer.add_graph(self.model, images)\n",
        "        model_writer.close()\n",
        "\n",
        "        train_loss = []\n",
        "        test_acc = []\n",
        "        zero_one_loss = []\n",
        "        zero_one_acc = []\n",
        "\n",
        "        print('Model with parameter count {}'.format(self.param_counts[self.current_count]))\n",
        "        print('-' * 10)\n",
        "\n",
        "        if self.cuda:\n",
        "            self.model = self.model.cuda()\n",
        "\n",
        "        for epoch in tqdm(range(self.max_epochs)):\n",
        "            # if epoch % 500 == 0:\n",
        "            #     print('Epoch {}/{}'.format(epoch + 1, self.max_epochs))\n",
        "            #     # print('-' * 10)\n",
        "\n",
        "            # Switches between training and testing sets\n",
        "            for phase in ['train', 'test']:\n",
        "\n",
        "                if phase == 'train':\n",
        "                    self.model.train()\n",
        "                    running_loss = 0.0\n",
        "                    running_zero_one_loss = 0.0\n",
        "                elif phase == 'test':\n",
        "                    self.model.eval()   # Set model to evaluate mode\n",
        "                    running_test_loss = 0.0\n",
        "                    running_zero_one_acc = 0.0\n",
        "\n",
        "                # Train/Test loop\n",
        "                for i, d in enumerate(self.data.dataloaders[phase], 0):\n",
        "\n",
        "                    inputs, labels = d\n",
        "\n",
        "                    if self.cuda:\n",
        "                        inputs = inputs.cuda()\n",
        "                        labels = labels.cuda()\n",
        "                    self.mlp_optim.zero_grad()\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        outputs = self.model.forward(inputs)\n",
        "                        small_outputs = self.predict_small(inputs)\n",
        "                        loss = self.loss(outputs, labels)\n",
        "                        # backward + optimize only if in training small_outputs), labels)\n",
        "                        # backward + optimize only if in training phase\n",
        "                        loss.backward()\n",
        "                        self.mlp_optim.step()\n",
        "                        one_hot = F.one_hot(torch.argmax(outputs, dim=1), 10)\n",
        "                        # for _ in range(1000):\n",
        "                        #   print(outputs[0], one_hot[0])\n",
        "                        zero_one_train = torch_zero_one_loss(self.num_votes/(self.num_votes + 4) * one_hot + 1/(self.num_votes + 4) * small_outputs, labels)\n",
        "                        running_zero_one_loss += zero_one_train.item() * inputs.size(0)\n",
        "                        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                    if phase == 'test':\n",
        "                        outputs = self.model.forward(inputs)\n",
        "                        small_outputs = self.predict_small(inputs)\n",
        "                        test_loss = self.loss(outputs, labels)\n",
        "                        one_hot = F.one_hot(torch.argmax(outputs, dim=1), 10)\n",
        "                        zero_one_test = torch_zero_one_loss(self.num_votes/(self.num_votes + 4) * one_hot + 1/(self.num_votes + 4) * small_outputs, labels)\n",
        "                        running_zero_one_acc += zero_one_test.item() * inputs.size(0)\n",
        "                        running_test_loss += test_loss.item() * inputs.size(0)\n",
        "\n",
        "                if phase == 'train' and self.post_flag == False:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            train_loss.append(running_loss/ self.data.dataset_sizes['train'])\n",
        "            test_acc.append(running_test_loss/ self.data.dataset_sizes['test'])\n",
        "            zero_one_loss.append(running_zero_one_loss/self.data.dataset_sizes['train'])\n",
        "            zero_one_acc.append(running_zero_one_acc/self.data.dataset_sizes['test'])\n",
        "\n",
        "            model_writer.add_scalar(f'Train-Loss/{self.hidden_layer_size} hidden units; {self.param_counts[self.current_count]}*{self.factor} total parameters',\n",
        "                              train_loss[-1],\n",
        "                             epoch)\n",
        "\n",
        "            model_writer.add_scalar(f'Test-Loss/{self.hidden_layer_size} hidden units; {self.param_counts[self.current_count]}*{self.factor} total parameters',\n",
        "                              test_acc[-1],\n",
        "                              epoch)\n",
        "\n",
        "            model_writer.add_scalar(f'Train-Loss/{self.hidden_layer_size} hidden units; {self.param_counts[self.current_count]}*{self.factor} total parameters (Zero-One)',\n",
        "                                    zero_one_loss[-1],\n",
        "                                    epoch)\n",
        "\n",
        "            model_writer.add_scalar(f'Test-Loss/{self.hidden_layer_size} hidden units; {self.param_counts[self.current_count]}*{self.factor} total parameters (Zero-One)',\n",
        "                                    zero_one_acc[-1],\n",
        "                                    epoch)\n",
        "\n",
        "\n",
        "            if (zero_one_loss[-1] == 0 or train_loss[-1] < 10**-4):\n",
        "                if self.generate_parameters:\n",
        "                    if self.post_flag:\n",
        "                        break\n",
        "\n",
        "                if self.param_counts[self.current_count] * self.factor < self.samples * self.data.num_classes:\n",
        "                    break\n",
        "\n",
        "        print('Train Loss: {:.4f}\\nTest Loss {:.4f}\\n{} Hidden Units'.format(zero_one_loss[-1], zero_one_acc[-1], self.hidden_layer_size))\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return self.model, train_loss, test_acc, zero_one_loss, zero_one_acc\n",
        "\n",
        "\n",
        "    def double_descent(self):\n",
        "        \"\"\"Uses the train and get_next_param_count methods\n",
        "        to train the same architecture with varying parameter\n",
        "        sizes. This method also keeps track of the final losses\n",
        "        of each model that is trained\n",
        "\n",
        "        ...\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : Models instance\n",
        "            The model object that will be trained with varying\n",
        "            parameter sizes\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        try:\n",
        "            os.makedirs(f'{self.num_votes}-ensemble-output')\n",
        "        except Exception as E:\n",
        "            print('Could not make mlp-output')\n",
        "            print(E)\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            shutil.rmtree(f'{self.num_votes}-ensemble-runs')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        dd_writer = SummaryWriter(f'{self.num_votes}-ensemble-runs/double-descent')\n",
        "        while self.current_count < len(self.param_counts):\n",
        "\n",
        "            _, train_loss, test_loss, zero_one_train, zero_one_test = self.train()\n",
        "            save_obj(self, f'{self.num_votes}-ensemble-model/{self.param_counts[self.current_count]}_width')\n",
        "\n",
        "            self.losses['train'] = np.append(self.losses['train'], train_loss[-1])\n",
        "            self.losses['test'] = np.append(self.losses['test'], test_loss[-1])\n",
        "            self.losses['zero_one_train'] = np.append(self.losses['zero_one_train'], zero_one_train[-1])\n",
        "            self.losses['zero_one_test'] = np.append(self.losses['zero_one_test'], zero_one_test[-1])\n",
        "\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Train',\n",
        "                                 self.losses['train'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Test',\n",
        "                                 self.losses['test'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Train (Zero-One)',\n",
        "                                 self.losses['zero_one_train'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Test (Zero-One)',\n",
        "                                 self.losses['zero_one_test'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            self.current_count += 1\n",
        "\n",
        "            if self.current_count < len(self.param_counts):\n",
        "                self.reinitialize_classifier()\n",
        "\n",
        "            np.save(f'{self.num_votes}-ensemble-output/train_loss.npy', self.losses['train'])\n",
        "            np.save(f'{self.num_votes}-ensemble-output/test_loss.npy', self.losses['test'])\n",
        "            np.save(f'{self.num_votes}-ensemble-output/zero_one_train.npy', self.losses['zero_one_train'])\n",
        "            np.save(f'{self.num_votes}-ensemble-output/zero_one_test.npy', self.losses['zero_one_test'])\n",
        "            np.save(f'{self.num_votes}-ensemble-output/parameter_counts', self.param_counts)\n",
        "\n",
        "        if not self.generate_parameters:\n",
        "\n",
        "            return {'train_loss': self.losses['train'],\n",
        "                    'test_loss': self.losses['test'],\n",
        "                    'zero_one_train': self.losses['zero_one_train'],\n",
        "                    'zero_one_test': self.losses['zero_one_test'],\n",
        "                    'parameter_counts': self.param_counts}\n",
        "\n",
        "\n",
        "        self.current_count -= 1\n",
        "        flag = False\n",
        "        while self.post_flag < 4:\n",
        "\n",
        "\n",
        "            next_ct, flag = get_next_param_count(self.param_counts,\n",
        "                                                 self.losses['test']/self.losses['test'].sum(),\n",
        "                                                 flag)\n",
        "            save_obj(self, f'{self.num_votes}-model/{next_ct}_width')\n",
        "            self.param_counts = np.append(self.param_counts, next_ct)\n",
        "            self.current_count += 1\n",
        "            self.reinitialize_classifier()\n",
        "\n",
        "            _, train_loss, test_loss, zero_one_train, zero_one_test = self.train()\n",
        "\n",
        "            self.losses['train'] = np.append(self.losses['train'], train_loss[-1])\n",
        "            self.losses['test'] = np.append(self.losses['test'], test_loss[-1])\n",
        "            self.losses['zero_one_train'] = np.append(self.losses['zero_one_train'], zero_one_train[-1])\n",
        "            self.losses['zero_one_test'] = np.append(self.losses['zero_one_test'], zero_one_test[-1])\n",
        "\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Train',\n",
        "                                 self.losses['train'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Test',\n",
        "                                 self.losses['test'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Train (Zero-One)',\n",
        "                                 self.losses['zero_one_train'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "            dd_writer.add_scalar('Ensemble-Double-Descent/Test (Zero-One)',\n",
        "                                 self.losses['zero_one_test'][-1],\n",
        "                                 self.param_counts[self.current_count])\n",
        "\n",
        "            if flag and (self.param_counts[-1] - self.param_counts[-2]) != 1:\n",
        "                print('Iterating Post Flag')\n",
        "                self.post_flag += 1\n",
        "                print(f'Post Flag {self.post_flag}')\n",
        "\n",
        "            np.save(f'{self.num_votes}-ensemble-output/train_loss.npy', self.losses['train'])\n",
        "            np.save(f'{self.num_votes}-ensemble-output/test_loss.npy', self.losses['test'])\n",
        "            np.save(f'{self.num_votes}-ensemble-output/zero_one_train.npy', self.losses['zero_one_train'])\n",
        "            np.save(f'{self.num_votes}-ensemble-output/zero_one_test.npy', self.losses['zero_one_test'])\n",
        "            np.save(f'{self.num_votes}-ensemble-output/parameter_counts', self.param_counts)\n",
        "\n",
        "        return {'train_loss': self.losses['train'],\n",
        "                'test_loss': self.losses['test'],\n",
        "                'zero_one_train': self.losses['zero_one_train'],\n",
        "                'zero_one_test': self.losses['zero_one_test'],\n",
        "                'parameter_counts': self.param_counts}"
      ],
      "metadata": {
        "id": "MPsaZs08Zmve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nPBH-5Q2EyP"
      },
      "outputs": [],
      "source": [
        "class SKLearnModels:\n",
        "    \"\"\"This class contains the attributes that all scikit-learn models\n",
        "    have in common. All scikit-learn models will inherit from this class\n",
        "\n",
        "    ...\n",
        "    Parameters (Not Attributes)\n",
        "    ---------------------------\n",
        "    dataset : str\n",
        "        A string that represents the dataset that the user wants to train\n",
        "        the model on. The current list is {MNIST}\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    dataset : np.array\n",
        "        The chosen dataset from the list {MNIST}\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, samples):\n",
        "\n",
        "        data_object = SKLearnData()\n",
        "        data_dict = {'MNIST': data_object.get_mnist}\n",
        "        X, y, X_val, y_val = data_dict[dataset](samples=samples)\n",
        "        self.dataset = {'X': X, 'y': y, 'X_val': X_val, 'y_val': y_val}\n",
        "\n",
        "\n",
        "class RandomForest(SKLearnModels):\n",
        "    \"\"\"A Random Forest wrapper that allows for variable numbers of trees\n",
        "    and maximum leaf nodes\n",
        "\n",
        "    ...\n",
        "    Parameters (Not Attributes)\n",
        "    ---------------------------\n",
        "    dataset : str\n",
        "        A string that represents the dataset that the user wants to train\n",
        "        the model on. The current list is {MNIST}\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    N_tree : int\n",
        "        The number of trees\n",
        "    N_max_leaves : int\n",
        "        The maximum number of leaf nodes on a tree\n",
        "    classifier : RandomForestClassifier\n",
        "        A scikit-learn random forest model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset='MNIST',\n",
        "                 N_tree=1,\n",
        "                 N_max_leaves=10,\n",
        "                 bootstrap=False,\n",
        "                 criterion='gini',\n",
        "                 samples=4000,\n",
        "                 leaves_limit=2010,\n",
        "                 tree_limit=30,\n",
        "                 leaves_iter=100,\n",
        "                 tree_iter=1):\n",
        "\n",
        "        self.dataset_name = dataset\n",
        "        super(RandomForest, self).__init__(dataset, samples)\n",
        "        self.N_tree = N_tree\n",
        "        self.samples = samples\n",
        "        self.N_max_leaves = N_max_leaves\n",
        "        self.bootstrap = bootstrap\n",
        "        self.criterion = criterion\n",
        "        self.leaves_limit = leaves_limit\n",
        "        self.tree_limit = tree_limit\n",
        "        self.leaves_iter = leaves_iter\n",
        "        self.tree_iter = tree_iter\n",
        "        print('Initializing RandomForest')\n",
        "        self.classifier = RandomForestClassifier(n_estimators=self.N_tree,\n",
        "                                                 bootstrap=self.bootstrap,\n",
        "                                                 criterion=self.criterion,\n",
        "                                                 max_leaf_nodes=self.N_max_leaves)\n",
        "\n",
        "    def reinitialize_classifier(self):\n",
        "        \"\"\"Helper function for double_descent method\"\"\"\n",
        "\n",
        "        self.classifier = RandomForestClassifier(n_estimators=self.N_tree,\n",
        "                                                 bootstrap=self.bootstrap,\n",
        "                                                 criterion=self.criterion,\n",
        "                                                 max_leaf_nodes=self.N_max_leaves)\n",
        "\n",
        "    def double_descent(self):\n",
        "        \"\"\"Exhibits double descent in random forest by increasing the\n",
        "        number of parameters (number of trees and leaf nodes) and training\n",
        "        each model to completion.\n",
        "\n",
        "\n",
        "        ...\n",
        "        Returns\n",
        "        -------\n",
        "        collected_data : dict\n",
        "            Dictionary of different losses and model attributes collected\n",
        "            throughout the training process. The keys are {'train_loss',\n",
        "            'zero_one_loss', 'mse_loss', 'leaf_sizes', 'trees'}\n",
        "        \"\"\"\n",
        "\n",
        "        leaf_sizes = []\n",
        "        trees = []\n",
        "\n",
        "        training_losses = np.array([])\n",
        "        zero_one_test_losses = np.array([])\n",
        "        mse_losses = np.array([])\n",
        "\n",
        "        while self.N_max_leaves <= self.leaves_limit:\n",
        "\n",
        "            self.classifier.fit(self.dataset['X'], self.dataset['y'])\n",
        "\n",
        "            train_loss = sk_zero_one_loss(\n",
        "                self.classifier.predict(self.dataset['X']), self.dataset['y'])\n",
        "\n",
        "            zero_one_test_loss = sk_zero_one_loss(\n",
        "                self.classifier.predict(self.dataset['X_val']), self.dataset['y_val'])\n",
        "\n",
        "            mse_loss = sk_mean_squared_error(\n",
        "                labels_to_vec(self.dataset['y_val']),\n",
        "                labels_to_vec(self.classifier.predict(self.dataset['X_val'])))\n",
        "            print(zero_one_test_loss)\n",
        "            training_losses = np.append(training_losses, train_loss)\n",
        "            zero_one_test_losses = np.append(zero_one_test_losses, zero_one_test_loss)\n",
        "            mse_losses = np.append(mse_losses, mse_loss)\n",
        "\n",
        "            leaf_sizes.append(self.N_max_leaves)\n",
        "            trees.append(self.N_tree)\n",
        "\n",
        "            self.N_max_leaves += self.leaves_iter\n",
        "            self.reinitialize_classifier()\n",
        "\n",
        "        self.N_max_leaves = self.N_max_leaves - self.leaves_iter\n",
        "        while self.N_tree <= self.tree_limit:\n",
        "\n",
        "            self.reinitialize_classifier()\n",
        "\n",
        "            self.classifier.fit(self.dataset['X'], self.dataset['y'])\n",
        "\n",
        "            train_loss = sk_zero_one_loss(\n",
        "                self.classifier.predict(self.dataset['X']), self.dataset['y'])\n",
        "\n",
        "            zero_one_test_loss = sk_zero_one_loss(\n",
        "                self.classifier.predict(self.dataset['X_val']), self.dataset['y_val'])\n",
        "            print(zero_one_test_loss)\n",
        "            mse_loss = sk_mean_squared_error(\n",
        "                labels_to_vec(self.dataset['y_val']),\n",
        "                labels_to_vec(self.classifier.predict(self.dataset['X_val'])))\n",
        "\n",
        "            training_losses = np.append(training_losses, train_loss)\n",
        "            zero_one_test_losses = np.append(zero_one_test_losses, zero_one_test_loss)\n",
        "            mse_losses = np.append(mse_losses, mse_loss)\n",
        "\n",
        "            leaf_sizes.append(self.N_max_leaves)\n",
        "            trees.append(self.N_tree)\n",
        "\n",
        "            self.N_tree += self.tree_iter\n",
        "\n",
        "\n",
        "        return {'train_loss': training_losses,\n",
        "                'zero_one_loss': zero_one_test_losses,\n",
        "                'mse_loss': mse_losses,\n",
        "                'leaf_sizes': np.array(leaf_sizes),\n",
        "                'trees': np.array(trees),\n",
        "                'samples': self.samples,\n",
        "                'dataset': self.dataset_name}\n",
        "\n",
        "class DecisionTree(SKLearnModels):\n",
        "    \"\"\"A Decision Tree wrapper that allows for variable numbers of trees\n",
        "    and maximum leaf nodes\n",
        "\n",
        "    ...\n",
        "    Parameters (Not Attributes)\n",
        "    ---------------------------\n",
        "    dataset : str\n",
        "        A string that represents the dataset that the user wants to train\n",
        "        the model on. The current list is {MNIST}\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    N_tree : int\n",
        "        The number of trees\n",
        "    N_max_leaves : int\n",
        "        The maximum number of leaf nodes on a tree\n",
        "    classifier : DecisionTreeClassifier\n",
        "        A scikit-learn decision tree model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset='MNIST',\n",
        "                 criterion='gini',\n",
        "                 N_max_leaves=10,\n",
        "                 max_depth=1,\n",
        "                 samples=4000,\n",
        "                 leaves_limit=2010,\n",
        "                 max_features=None,\n",
        "                 leaves_iter=10,\n",
        "                 depth_iter=10,\n",
        "                 depth_limit=1000):\n",
        "\n",
        "        self.dataset_name = dataset\n",
        "        super(DecisionTree, self).__init__(dataset, samples)\n",
        "        self.samples = samples\n",
        "        self.N_max_leaves = N_max_leaves\n",
        "        self.criterion = criterion\n",
        "        self.max_features = max_features\n",
        "        self.leaves_iter = leaves_iter\n",
        "        self.leaves_limit = leaves_limit\n",
        "        self.max_depth = max_depth\n",
        "        self.depth_iter = depth_iter\n",
        "        self.depth_limit = depth_limit\n",
        "        print('Initializing DecisionTree')\n",
        "        self.classifier = DecisionTreeClassifier(criterion=self.criterion,\n",
        "                                                 max_leaf_nodes=self.N_max_leaves,\n",
        "                                                 max_features=self.max_features,\n",
        "                                                 max_depth=self.max_depth)\n",
        "\n",
        "    def reinitialize_classifier(self):\n",
        "        \"\"\"Helper function for double_descent method\"\"\"\n",
        "\n",
        "        self.classifier = DecisionTreeClassifier(criterion=self.criterion,\n",
        "                                                 max_leaf_nodes=self.N_max_leaves,\n",
        "                                                 max_features=self.max_features,\n",
        "                                                 max_depth=self.max_depth)\n",
        "\n",
        "    def double_descent(self):\n",
        "        \"\"\"Exhibits double descent in random forest by increasing the\n",
        "        number of parameters (number of trees and leaf nodes) and training\n",
        "        each model to completion.\n",
        "\n",
        "\n",
        "        ...\n",
        "        Returns\n",
        "        -------\n",
        "        collected_data : dict\n",
        "            Dictionary of different losses and model attributes collected\n",
        "            throughout the training process. The keys are {'train_loss',\n",
        "            'zero_one_loss', 'mse_loss', 'leaf_sizes', 'trees'}\n",
        "        \"\"\"\n",
        "\n",
        "        leaf_sizes = []\n",
        "        depths = []\n",
        "\n",
        "        training_losses = np.array([])\n",
        "        zero_one_test_losses = np.array([])\n",
        "        mse_losses = np.array([])\n",
        "\n",
        "        while self.N_max_leaves <= self.leaves_limit:\n",
        "            print(self.N_max_leaves)\n",
        "            self.classifier.fit(self.dataset['X'], self.dataset['y'])\n",
        "\n",
        "            train_loss = sk_zero_one_loss(\n",
        "                self.classifier.predict(self.dataset['X']), self.dataset['y'])\n",
        "\n",
        "            zero_one_test_loss = sk_zero_one_loss(\n",
        "                self.classifier.predict(self.dataset['X_val']), self.dataset['y_val'])\n",
        "\n",
        "            mse_loss = sk_mean_squared_error(\n",
        "                labels_to_vec(self.dataset['y_val']),\n",
        "                labels_to_vec(self.classifier.predict(self.dataset['X_val'])))\n",
        "\n",
        "            training_losses = np.append(training_losses, train_loss)\n",
        "            zero_one_test_losses = np.append(zero_one_test_losses, zero_one_test_loss)\n",
        "            mse_losses = np.append(mse_losses, mse_loss)\n",
        "            print(zero_one_test_loss)\n",
        "            leaf_sizes.append(self.N_max_leaves)\n",
        "            depths.append(self.max_depth)\n",
        "\n",
        "            self.N_max_leaves += self.leaves_iter\n",
        "            self.reinitialize_classifier()\n",
        "\n",
        "        self.N_max_leaves -= self.leaves_limit\n",
        "\n",
        "        while self.max_depth <= self.depth_limit:\n",
        "            self.reinitialize_classifier()\n",
        "\n",
        "            self.classifier.fit(self.dataset['X'], self.dataset['y'])\n",
        "\n",
        "            train_loss = sk_zero_one_loss(\n",
        "                self.classifier.predict(self.dataset['X']), self.dataset['y'])\n",
        "\n",
        "            zero_one_test_loss = sk_zero_one_loss(\n",
        "                self.classifier.predict(self.dataset['X_val']), self.dataset['y_val'])\n",
        "            print(zero_one_test_loss)\n",
        "            mse_loss = sk_mean_squared_error(\n",
        "                labels_to_vec(self.dataset['y_val']),\n",
        "                labels_to_vec(self.classifier.predict(self.dataset['X_val'])))\n",
        "\n",
        "            training_losses = np.append(training_losses, train_loss)\n",
        "            zero_one_test_losses = np.append(zero_one_test_losses, zero_one_test_loss)\n",
        "            mse_losses = np.append(mse_losses, mse_loss)\n",
        "\n",
        "            leaf_sizes.append(self.N_max_leaves)\n",
        "            depths.append(self.max_depth)\n",
        "\n",
        "            self.max_depth += self.depth_iter\n",
        "\n",
        "\n",
        "        return {'train_loss': training_losses,\n",
        "                'zero_one_loss': zero_one_test_losses,\n",
        "                'mse_loss': mse_losses,\n",
        "                'leaf_sizes': np.array(leaf_sizes),\n",
        "                'depths': np.array(depths),\n",
        "                'samples': self.samples,\n",
        "                'dataset': self.dataset_name}\n",
        "\n",
        "class AdaBoost(SKLearnModels):\n",
        "    \"\"\"An AdaBoost wrapper that allows for variable numbers of trees\n",
        "    and maximum leaf nodes\n",
        "\n",
        "    ...\n",
        "    Parameters (Not Attributes)\n",
        "    ---------------------------\n",
        "    dataset : str\n",
        "        A string that represents the dataset that the user wants to train\n",
        "        the model on. The current list is {MNIST}\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    N_tree : int\n",
        "        The number of trees\n",
        "    N_max_leaves : int\n",
        "        The maximum number of leaf nodes on a tree\n",
        "    classifier : AdaBoostClassifier\n",
        "        A scikit-learn random forest model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset='MNIST',\n",
        "                 N_tree=1,\n",
        "                 N_forest=1,\n",
        "                 N_max_leaves=10,\n",
        "                 bootstrap=False,\n",
        "                 criterion='gini',\n",
        "                 samples=10000,\n",
        "                 tree_limit=50,\n",
        "                 forest_limit=30,\n",
        "                 tree_iter=1,\n",
        "                 forest_iter=1,\n",
        "                 max_features='sqrt',\n",
        "                 learning_rate=0.85):\n",
        "\n",
        "        self.dataset_name = dataset\n",
        "        super(AdaBoost, self).__init__(dataset, samples)\n",
        "        self.N_tree = N_tree\n",
        "        self.N_forest = N_forest\n",
        "        self.samples = samples\n",
        "        self.N_max_leaves = N_max_leaves\n",
        "        self.bootstrap = bootstrap\n",
        "        self.criterion = criterion\n",
        "        self.forest_limit = forest_limit\n",
        "        self.tree_limit = tree_limit\n",
        "        self.forest_iter = forest_iter\n",
        "        self.tree_iter = tree_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_features = max_features\n",
        "        print('Initializing AdaBoost')\n",
        "        self.estimator = DecisionTreeClassifier(criterion=self.criterion,\n",
        "                                                max_leaf_nodes=self.N_max_leaves,\n",
        "                                                max_features = self.max_features)\n",
        "\n",
        "        self.classifier = [AdaBoostClassifier(estimator=self.estimator,\n",
        "                                             n_estimators=self.N_tree,\n",
        "                                             learning_rate=self.learning_rate) for _ in range(self.N_forest)]\n",
        "\n",
        "    def reinitialize_classifier(self):\n",
        "        \"\"\"Helper function for double_descent method\"\"\"\n",
        "        if self.N_forest == len(self.classifier):\n",
        "          self.classifier = [AdaBoostClassifier(estimator=self.estimator,\n",
        "                                              n_estimators=self.N_tree,\n",
        "                                              learning_rate=self.learning_rate) for _ in range(self.N_forest)]\n",
        "        else:\n",
        "          self.classifier.extend([AdaBoostClassifier(estimator=self.estimator,\n",
        "                                                     n_estimators=self.N_tree,\n",
        "                                                     learning_rate=self.learning_rate) for _ in range(self.N_forest - len(self.classifier))])\n",
        "\n",
        "    def double_descent(self):\n",
        "        \"\"\"Exhibits double descent in random forest by increasing the\n",
        "        number of parameters (number of trees and leaf nodes) and training\n",
        "        each model to completion.\n",
        "\n",
        "\n",
        "        ...\n",
        "        Returns\n",
        "        -------\n",
        "        collected_data : dict\n",
        "            Dictionary of different losses and model attributes collected\n",
        "            throughout the training process. The keys are {'train_loss',\n",
        "            'zero_one_loss', 'mse_loss', 'leaf_sizes', 'trees'}\n",
        "        \"\"\"\n",
        "        trees = []\n",
        "        forests = []\n",
        "\n",
        "        training_losses = np.array([])\n",
        "        zero_one_test_losses = np.array([])\n",
        "        mse_losses = np.array([])\n",
        "\n",
        "        while self.N_tree <= self.tree_limit:\n",
        "            for i in range(self.N_forest):\n",
        "              self.classifier[i].fit(self.dataset['X'], self.dataset['y'])\n",
        "\n",
        "            train_loss = np.mean([sk_zero_one_loss(\n",
        "                self.classifier[i].predict(self.dataset['X']), self.dataset['y']) for i in range(self.N_forest)])\n",
        "\n",
        "            zero_one_test_loss = np.mean([sk_zero_one_loss(\n",
        "                self.classifier[i].predict(self.dataset['X_val']), self.dataset['y_val']) for i in range(self.N_forest)])\n",
        "\n",
        "            mse_loss = np.mean([sk_mean_squared_error(\n",
        "                labels_to_vec(self.dataset['y_val']),\n",
        "                labels_to_vec(self.classifier[i].predict(self.dataset['X_val']))) for i in range(self.N_forest)])\n",
        "            print(self.N_tree, zero_one_test_loss)\n",
        "            training_losses = np.append(training_losses, train_loss)\n",
        "            zero_one_test_losses = np.append(zero_one_test_losses, zero_one_test_loss)\n",
        "            mse_losses = np.append(mse_losses, mse_loss)\n",
        "\n",
        "            forests.append(self.N_forest)\n",
        "            trees.append(self.N_tree)\n",
        "\n",
        "            self.N_tree += self.tree_iter\n",
        "            self.reinitialize_classifier()\n",
        "\n",
        "        self.N_tree = self.N_tree - self.tree_iter\n",
        "        while self.N_forest <= self.forest_limit:\n",
        "            self.reinitialize_classifier()\n",
        "\n",
        "            for i in range(self.N_forest - self.forest_iter, self.N_forest):\n",
        "              self.classifier[i].fit(self.dataset['X'], self.dataset['y'])\n",
        "            predictions_train = stats.mode(np.array(\n",
        "                [self.classifier[i].predict(self.dataset['X']) for i in range(self.N_forest)]).astype(int), keepdims=True)[0].reshape(-1).astype(str)\n",
        "\n",
        "            predictions_test = stats.mode(np.array(\n",
        "                [self.classifier[i].predict(self.dataset['X_val']) for i in range(self.N_forest)]).astype(int), keepdims=True)[0].reshape(-1).astype(str)\n",
        "\n",
        "            train_loss = sk_zero_one_loss(predictions_train, self.dataset['y'])\n",
        "\n",
        "            zero_one_test_loss = sk_zero_one_loss(predictions_test, self.dataset['y_val'])\n",
        "\n",
        "            mse_loss = sk_mean_squared_error(\n",
        "                labels_to_vec(self.dataset['y_val']),\n",
        "                labels_to_vec(predictions_test))\n",
        "            print(self.N_forest, zero_one_test_loss)\n",
        "            training_losses = np.append(training_losses, train_loss)\n",
        "            zero_one_test_losses = np.append(zero_one_test_losses, zero_one_test_loss)\n",
        "            mse_losses = np.append(mse_losses, mse_loss)\n",
        "\n",
        "            forests.append(self.N_forest)\n",
        "            trees.append(self.N_tree)\n",
        "\n",
        "            self.N_forest += self.forest_iter\n",
        "\n",
        "\n",
        "        return {'train_loss': training_losses,\n",
        "                'zero_one_loss': zero_one_test_losses,\n",
        "                'mse_loss': mse_losses,\n",
        "                'forests': np.array(forests),\n",
        "                'trees': np.array(trees),\n",
        "                'samples': self.samples,\n",
        "                'dataset': self.dataset_name}\n",
        "\n",
        "class L2Boost(SKLearnModels):\n",
        "    \"\"\"An AdaBoost wrapper that allows for variable numbers of trees\n",
        "    and maximum leaf nodes\n",
        "\n",
        "    ...\n",
        "    Parameters (Not Attributes)\n",
        "    ---------------------------\n",
        "    dataset : str\n",
        "        A string that represents the dataset that the user wants to train\n",
        "        the model on. The current list is {MNIST}\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    N_tree : int\n",
        "        The number of trees\n",
        "    N_max_leaves : int\n",
        "        The maximum number of leaf nodes on a tree\n",
        "    classifier : AdaBoostClassifier\n",
        "        A scikit-learn random forest model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset='MNIST',\n",
        "                 N_tree=1,\n",
        "                 N_forest=1,\n",
        "                 N_max_leaves=10,\n",
        "                 criterion='friedman_mse',\n",
        "                 samples=10000,\n",
        "                 tree_limit=50,\n",
        "                 forest_limit=30,\n",
        "                 tree_iter=1,\n",
        "                 forest_iter=1,\n",
        "                 max_features='sqrt',\n",
        "                 learning_rate=0.85):\n",
        "\n",
        "        self.dataset_name = dataset\n",
        "        super(L2Boost, self).__init__(dataset, samples)\n",
        "        self.N_tree = N_tree\n",
        "        self.N_forest = N_forest\n",
        "        self.samples = samples\n",
        "        self.N_max_leaves = N_max_leaves\n",
        "        self.criterion = criterion\n",
        "        self.forest_limit = forest_limit\n",
        "        self.tree_limit = tree_limit\n",
        "        self.forest_iter = forest_iter\n",
        "        self.tree_iter = tree_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_features = max_features\n",
        "        print('Initializing L2-Boosting Tree')\n",
        "\n",
        "        self.classifier = [GradientBoostingClassifier(criterion=self.criterion,\n",
        "                                                      learning_rate=self.learning_rate,\n",
        "                                                      n_estimators=self.N_tree,\n",
        "                                                      max_features=self.max_features,\n",
        "                                                      max_leaf_nodes=self.N_max_leaves) for _ in range(self.N_forest)]\n",
        "\n",
        "    def reinitialize_classifier(self):\n",
        "        \"\"\"Helper function for double_descent method\"\"\"\n",
        "        if self.N_forest == len(self.classifier):\n",
        "          self.classifier = [GradientBoostingClassifier(criterion=self.criterion,\n",
        "                                                      learning_rate=self.learning_rate,\n",
        "                                                      n_estimators=self.N_tree,\n",
        "                                                      max_features=self.max_features,\n",
        "                                                      max_leaf_nodes=self.N_max_leaves) for _ in range(self.N_forest)]\n",
        "        else:\n",
        "          self.classifier.extend([GradientBoostingClassifier(criterion=self.criterion,\n",
        "                                                      learning_rate=self.learning_rate,\n",
        "                                                      n_estimators=self.N_tree,\n",
        "                                                      max_features=self.max_features,\n",
        "                                                      max_leaf_nodes=self.N_max_leaves) for _ in range(self.N_forest - len(self.classifier))])\n",
        "\n",
        "    def double_descent(self):\n",
        "        \"\"\"Exhibits double descent in random forest by increasing the\n",
        "        number of parameters (number of trees and leaf nodes) and training\n",
        "        each model to completion.\n",
        "\n",
        "\n",
        "        ...\n",
        "        Returns\n",
        "        -------\n",
        "        collected_data : dict\n",
        "            Dictionary of different losses and model attributes collected\n",
        "            throughout the training process. The keys are {'train_loss',\n",
        "            'zero_one_loss', 'mse_loss', 'leaf_sizes', 'trees'}\n",
        "        \"\"\"\n",
        "        trees = []\n",
        "        forests = []\n",
        "\n",
        "        training_losses = np.array([])\n",
        "        zero_one_test_losses = np.array([])\n",
        "        mse_losses = np.array([])\n",
        "\n",
        "        while self.N_tree <= self.tree_limit:\n",
        "            for i in range(self.N_forest):\n",
        "              self.classifier[i].fit(self.dataset['X'], self.dataset['y'])\n",
        "\n",
        "            train_loss = np.mean([sk_zero_one_loss(\n",
        "                self.classifier[i].predict(self.dataset['X']), self.dataset['y']) for i in range(self.N_forest)])\n",
        "\n",
        "            zero_one_test_loss = np.mean([sk_zero_one_loss(\n",
        "                self.classifier[i].predict(self.dataset['X_val']), self.dataset['y_val']) for i in range(self.N_forest)])\n",
        "\n",
        "            mse_loss = np.mean([sk_mean_squared_error(\n",
        "                labels_to_vec(self.dataset['y_val']),\n",
        "                labels_to_vec(self.classifier[i].predict(self.dataset['X_val']))) for i in range(self.N_forest)])\n",
        "            print(self.N_tree, zero_one_test_loss)\n",
        "            training_losses = np.append(training_losses, train_loss)\n",
        "            zero_one_test_losses = np.append(zero_one_test_losses, zero_one_test_loss)\n",
        "            mse_losses = np.append(mse_losses, mse_loss)\n",
        "\n",
        "            forests.append(self.N_forest)\n",
        "            trees.append(self.N_tree)\n",
        "\n",
        "            self.N_tree += self.tree_iter\n",
        "            self.reinitialize_classifier()\n",
        "\n",
        "        self.N_tree = self.N_tree - self.tree_iter\n",
        "        while self.N_forest <= self.forest_limit:\n",
        "            self.reinitialize_classifier()\n",
        "\n",
        "            for i in range(self.N_forest - self.forest_iter, self.N_forest):\n",
        "              self.classifier[i].fit(self.dataset['X'], self.dataset['y'])\n",
        "            predictions_train = stats.mode(np.array(\n",
        "                [self.classifier[i].predict(self.dataset['X']) for i in range(self.N_forest)]).astype(int), keepdims=True)[0].reshape(-1).astype(str)\n",
        "\n",
        "            predictions_test = stats.mode(np.array(\n",
        "                [self.classifier[i].predict(self.dataset['X_val']) for i in range(self.N_forest)]).astype(int), keepdims=True)[0].reshape(-1).astype(str)\n",
        "\n",
        "            train_loss = sk_zero_one_loss(predictions_train, self.dataset['y'])\n",
        "\n",
        "            zero_one_test_loss = sk_zero_one_loss(predictions_test, self.dataset['y_val'])\n",
        "\n",
        "            mse_loss = sk_mean_squared_error(\n",
        "                labels_to_vec(self.dataset['y_val']),\n",
        "                labels_to_vec(predictions_test))\n",
        "            print(self.N_forest, zero_one_test_loss)\n",
        "            training_losses = np.append(training_losses, train_loss)\n",
        "            zero_one_test_losses = np.append(zero_one_test_losses, zero_one_test_loss)\n",
        "            mse_losses = np.append(mse_losses, mse_loss)\n",
        "\n",
        "            forests.append(self.N_forest)\n",
        "            trees.append(self.N_tree)\n",
        "\n",
        "            self.N_forest += self.forest_iter\n",
        "\n",
        "\n",
        "        return {'train_loss': training_losses,\n",
        "                'zero_one_loss': zero_one_test_losses,\n",
        "                'mse_loss': mse_losses,\n",
        "                'forests': np.array(forests),\n",
        "                'trees': np.array(trees),\n",
        "                'samples': self.samples,\n",
        "                'dataset': self.dataset_name}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "a-toyhWr0CMs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsxtMXSqTd_M"
      },
      "outputs": [],
      "source": [
        "model = MultilayerPerceptron(cuda=True,\n",
        "                             loss='CrossEntropy',\n",
        "                             param_counts=np.array([3, 4, 7, 10, 15, 20, 23, 27, 31, 32, 33, 34, 36, 38, 40, 41, 42, 43, 44, 60, 80, 100, 150, 300, 800]),\n",
        "                             generate_parameters=False,\n",
        "                             max_epochs=2000,\n",
        "                             scheduler_step_size=500,\n",
        "                             batch_size=128,\n",
        "                             seed=0,\n",
        "                             reuse_weights=True)\n",
        "outs = model.double_descent()\n",
        "save_obj(outs, 'mlp-dd-experiment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWoyVyvXgkh6"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81X7Q3n-giES"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir mlp-runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGLg2Dt_V5IY",
        "outputId": "ce4cc2cc-ec75-4e54-c40c-a033c6b95d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/\")\n",
        "!cp -r data '/content/drive/My Drive/6.s898 Project/Project'\n",
        "!cp -r mlp-output '/content/drive/My Drive/6.s898 Project/Project'\n",
        "!cp -r mlp-runs '/content/drive/My Drive/6.s898 Project/Project'\n",
        "!cp -r mlp-model '/content/drive/My Drive/6.s898 Project/Project'\n",
        "!cp mlp-dd-experiment.pkl '/content/drive/My Drive/6.s898 Project/Project'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir mlp-100-epochs"
      ],
      "metadata": {
        "id": "hnMp1-sJG652"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/mlp-100-epochs')\n",
        "!mkdir mlp-model"
      ],
      "metadata": {
        "id": "4WQ-XLH2HGoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_model = MultilayerPerceptron(cuda=True,\n",
        "                             loss='CrossEntropy',\n",
        "                             param_counts=np.array([3, 4, 7, 10, 15, 20, 23, 27, 31, 32, 33, 34, 36, 38, 40, 41, 42, 43, 44, 60, 80, 100, 150, 300, 800]),\n",
        "                             generate_parameters=False,\n",
        "                             max_epochs=100,\n",
        "                             scheduler_step_size=50,\n",
        "                             batch_size=128,\n",
        "                             seed=0,\n",
        "                             reuse_weights=True)\n",
        "small_outs = small_model.double_descent()\n",
        "save_obj(small_outs, 'mlp-dd-experiment')"
      ],
      "metadata": {
        "id": "FNmX27MkHXFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir mlp-runs --port 6898"
      ],
      "metadata": {
        "id": "jffmXS8VPfbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content')\n",
        "!cp -r mlp-100-epochs '/content/drive/My Drive/6.s898 Project/Project'"
      ],
      "metadata": {
        "id": "R7K26j0hQKG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/')\n",
        "!rm -r mlp-500-epochs"
      ],
      "metadata": {
        "id": "hgRS7ypTSJGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir mlp-500-epochs\n",
        "os.chdir('/content/mlp-500-epochs')\n",
        "!mkdir mlp-model\n",
        "medium_model = MultilayerPerceptron(cuda=True,\n",
        "                             loss='CrossEntropy',\n",
        "                             param_counts=np.array([3, 4, 7, 10, 15, 20, 23, 27, 31, 32, 33, 34, 36, 38, 40, 41, 42, 43, 44, 60, 80, 100, 150, 300, 800]),\n",
        "                             generate_parameters=False,\n",
        "                             max_epochs=500,\n",
        "                             scheduler_step_size=100,\n",
        "                             batch_size=128,\n",
        "                             seed=0,\n",
        "                             reuse_weights=True)\n",
        "\n",
        "medium_outs = medium_model.double_descent()\n",
        "save_obj(medium_outs, 'mlp-dd-experiment')"
      ],
      "metadata": {
        "id": "SBGEU1FcQfxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir mlp-runs --port 8008"
      ],
      "metadata": {
        "id": "AkHn8PGku8o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.chdir('/content')\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# os.chdir(\"/content/\")\n",
        "# !cp -r mlp-500-epochs '/content/drive/My Drive/6.s898 Project/Project'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7lTZn0Fu_-l",
        "outputId": "52524795-168e-41c1-c722-5cf6d5551249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!rm -r ensemble-output\n",
        "!rm -r ensemble-runs\n",
        "!rm -r ensemble-model\n",
        "!mkdir ensemble-model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMbrjvkkC_XN",
        "outputId": "9289ea3e-bb98-43ad-d3a7-09d73937ab97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'ensemble-output': No such file or directory\n",
            "rm: cannot remove 'ensemble-runs': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = EnsembleMLP(cuda=True,\n",
        "                    loss='CrossEntropy',\n",
        "                    param_counts=np.array([3, 4, 7, 10, 15, 20, 23, 27, 31, 32, 33, 34, 36, 38, 40, 41, 42, 43, 44, 60, 80, 100, 150, 300, 800]),\n",
        "                    generate_parameters=False,\n",
        "                    max_epochs=100,\n",
        "                    scheduler_step_size=500,\n",
        "                    batch_size=128,\n",
        "                    seed=0,\n",
        "                    reuse_weights=True,\n",
        "                    num_models=5)\n",
        "outs = model.double_descent()\n",
        "save_obj(outs, 'ensemble-dd-experiment')"
      ],
      "metadata": {
        "id": "840Hp4sSmo-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill 25990\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ensemble-runs --port 2003"
      ],
      "metadata": {
        "id": "4wa0u_y1PWGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/6.s898 Project/Project\")\n",
        "!mkdir ensemble-100-epochs\n",
        "os.chdir(\"/content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOAZ-8USQKBA",
        "outputId": "d734be11-8724-4c8c-956a-1f42f74a96a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content\")\n",
        "!cp -r data \"/content/drive/My Drive/6.s898 Project/Project/ensemble-100-epochs\"\n",
        "!cp -r ensemble-model \"/content/drive/My Drive/6.s898 Project/Project/ensemble-100-epochs\"\n",
        "!cp -r ensemble-output \"/content/drive/My Drive/6.s898 Project/Project/ensemble-100-epochs\"\n",
        "!cp -r ensemble-runs \"/content/drive/My Drive/6.s898 Project/Project/ensemble-100-epochs\"\n",
        "!cp ensemble-dd-experiment.pkl \"/content/drive/My Drive/6.s898 Project/Project/ensemble-100-epochs\""
      ],
      "metadata": {
        "id": "U99IzNxaQrHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()\n",
        "!mkdir ensemble-500-epochs\n",
        "os.chdir(\"/content/ensemble-500-epochs\")"
      ],
      "metadata": {
        "id": "30YuCJyYSPOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo rm -r data\n",
        "!sudo rm -r ensemble-runs\n",
        "!mkdir ensemble-model"
      ],
      "metadata": {
        "id": "LQ456GXZREZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "med_model = EnsembleMLP(cuda=True,\n",
        "                    loss='CrossEntropy',\n",
        "                    param_counts=np.array([3, 4, 7, 10, 15, 20, 23, 27, 31, 32, 33, 34, 36, 38, 40, 41, 42, 43, 44, 60, 80, 100, 150, 300, 800]),\n",
        "                    generate_parameters=False,\n",
        "                    max_epochs=500,\n",
        "                    scheduler_step_size=500,\n",
        "                    batch_size=128,\n",
        "                    seed=0,\n",
        "                    reuse_weights=True,\n",
        "                    num_models=5)\n",
        "med_outs = med_model.double_descent()\n",
        "save_obj(med_outs, 'ensemble-dd-experiment')"
      ],
      "metadata": {
        "id": "l9bdD2PyUNFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ensemble-runs --port 1210"
      ],
      "metadata": {
        "id": "K-Z4utvixzhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content\")\n",
        "!cp -r ensemble-500-epochs \"/content/drive/My Drive/6.s898 Project/Project/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKOXla4pzltx",
        "outputId": "f426de57-d34e-4d1c-e2fa-f13701afb856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_obj('ensemble-500-epochs/ensemble-model/100_width')\n",
        "model"
      ],
      "metadata": {
        "id": "_IHEe6S_08gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content')"
      ],
      "metadata": {
        "id": "X_rItX0wzDM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r '/content/drive/My Drive/6.s898 Project/Project/mlp-100-epochs' '/content'"
      ],
      "metadata": {
        "id": "n-OAOAEyzq5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir scikit_learn_data\n",
        "def generate_set(samples):\n",
        "      data_object = SKLearnData()\n",
        "      data_dict = {'MNIST': data_object.get_mnist}\n",
        "      X, y, X_val, y_val = data_dict['MNIST'](samples=samples)\n",
        "      return {'X': X, 'y': y, 'X_val': X_val, 'y_val': y_val}\n",
        "dataset = generate_set(10000)\n",
        "model1 = LogisticRegression(C=64, max_iter=10000)\n",
        "model2 = RandomForestClassifier(n_estimators=10, max_leaf_nodes=2000, criterion='gini')\n",
        "model3 = DecisionTreeClassifier(criterion='gini', max_leaf_nodes=2000, max_features=392, max_depth=100)\n",
        "model4 = [GradientBoostingClassifier(criterion='friedman_mse', learning_rate=0.85, n_estimators=20, max_features='sqrt', max_leaf_nodes=10) for _ in range(5)]\n",
        "model1.fit(dataset['X'], dataset['y'])\n",
        "model2.fit(dataset['X'], dataset['y'])\n",
        "model3.fit(dataset['X'], dataset['y'])\n",
        "for model in model4:\n",
        "      model.fit(dataset['X'], dataset['y'])"
      ],
      "metadata": {
        "id": "YQ_ierhH0JBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = RandomForestClassifier(n_estimators=10, max_leaf_nodes=2000, criterion='gini')\n",
        "model3 = DecisionTreeClassifier(criterion='gini', max_leaf_nodes=2000, max_depth=100)\n",
        "model4 = [GradientBoostingClassifier(criterion='friedman_mse', learning_rate=0.85, n_estimators=20, max_features='sqrt', max_leaf_nodes=10) for _ in range(5)]\n",
        "model2.fit(dataset['X'], dataset['y'])\n",
        "model3.fit(dataset['X'], dataset['y'])\n",
        "for model in model4:\n",
        "      model.fit(dataset['X'], dataset['y'])"
      ],
      "metadata": {
        "id": "zmSNUWP-PtFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/Strong-Classifier-100-Epochs\")\n",
        "!rm -r ensemble-100-epochs-2-votes"
      ],
      "metadata": {
        "id": "YYHOc-If5zaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2, 4):\n",
        "  !mkdir -p \"ensemble-100-epochs-$i-votes\"\n",
        "  os.chdir(f'ensemble-100-epochs-{i}-votes')\n",
        "  !mkdir -p \"$i-ensemble-model\"\n",
        "  model = WeakEnsemble(cuda=True,\n",
        "                      loss='CrossEntropy',\n",
        "                      param_counts=np.array([3, 4, 7, 10, 15, 20, 23, 27, 31, 32, 33, 34, 36, 38, 40, 41, 42, 43, 44, 60, 80, 100, 150, 300, 800]),\n",
        "                      generate_parameters=False,\n",
        "                      max_epochs=100,\n",
        "                      scheduler_step_size=500,\n",
        "                      batch_size=128,\n",
        "                      seed=0,\n",
        "                      reuse_weights=True,\n",
        "                      num_votes=i,\n",
        "                      model1=model1,\n",
        "                      model2=model2,\n",
        "                      model3=model3,\n",
        "                      model4=model4)\n",
        "  outs = model.double_descent()\n",
        "  save_obj(outs, f'ensemble-dd-experiment')\n",
        "  os.chdir('/content/Strong-Classifier-100-Epochs')"
      ],
      "metadata": {
        "id": "ggbbszN83kx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/\")\n",
        "!cp -r Strong-Classifier-100-Epochs \"/content/drive/MyDrive/6.s898 Project/Project\""
      ],
      "metadata": {
        "id": "s9L45CzTL36g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/Strong-Classifier-100-Epochs\")\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ensemble-100-epochs-3-votes/3-ensemble-runs --port 1999"
      ],
      "metadata": {
        "id": "9_5JSPfijtyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r ensemble-100-epochs-1-votes\n",
        "!rm -r ensemble-100-epochs-2-votes\n",
        "!rm -r ensemble-100-epochs-3-votes\n",
        "!rm -r ensemble-100-epochs-4-votes"
      ],
      "metadata": {
        "id": "sGMqjtjgmSUp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}